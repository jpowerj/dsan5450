<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="36-313, Fall 2021">
<meta name="dcterms.date" content="2021-10-21">

<title>Algorithmic Fairness (Lecture 15)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="lecture-24_files/libs/clipboard/clipboard.min.js"></script>
<script src="lecture-24_files/libs/quarto-html/quarto.js"></script>
<script src="lecture-24_files/libs/quarto-html/popper.min.js"></script>
<script src="lecture-24_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="lecture-24_files/libs/quarto-html/anchor.min.js"></script>
<link href="lecture-24_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="lecture-24_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="lecture-24_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="lecture-24_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="lecture-24_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Algorithmic Fairness (Lecture 15)</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>36-313, Fall 2021 </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 21, 2021</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>When we talk about <strong>classification</strong> problem, we always mean that we’re trying to predict a categorical, usually binary, <strong>label</strong>, <strong>outcome</strong> or <strong>class</strong> <span class="math inline">\(Y\)</span> from <strong>features</strong> <span class="math inline">\(X\)</span> (which may or may not be categorical).</p>
<p>We’ll be a little more telegraphic today than usual.</p>
<section id="protected-attributes" class="level1">
<h1>“Protected attributes”</h1>
<ul>
<li>Legally mandated in some contexts
<ul>
<li>US law generally prohibits discrimination in employment, housing, lending, education, etc., on the basis of race, ethincity, sex, religion, national origin, or age
<ul>
<li>See our previous discussions of “disparate treatment” and “disparate impact”</li>
<li>OTOH there’s nothing in US law against (for example) discrimination by caste</li>
</ul></li>
</ul></li>
<li>Arguably <em>ethically</em> mandated everywhere</li>
</ul>
</section>
<section id="some-notions-of-fairness-for-classification" class="level1">
<h1>Some notions of “fairness” for classification</h1>
<ol type="1">
<li>Don’t use protected features <em>directly</em>
<ol type="a">
<li>Sometimes called “anti-classification”</li>
<li>What about strongly-associated unprotected features?</li>
</ol></li>
<li>Have equal error rates across groups
<ol type="a">
<li>Sometimes called “classification parity”</li>
<li>Which error measures, exactly?</li>
</ol></li>
<li>Calibration: everyone with the same score should have the same actual probability of <span class="math inline">\(Y=1\)</span>, regardless of group
<ol type="a">
<li>Conditional independence of <span class="math inline">\(Y\)</span> from protected attribute given score</li>
<li>This is a very weak requirement (which some systems nonetheless manage to violate…)</li>
</ol></li>
</ol>
<section id="concrete-example-pretrial-detention-recidivism-prediction" class="level2">
<h2 class="anchored" data-anchor-id="concrete-example-pretrial-detention-recidivism-prediction">Concrete example: Pretrial detention, recidivism prediction</h2>
<ul>
<li><em>You</em> don’t get arrested, your screw-up cousin Archie gets arrested</li>
<li>Court decides whether or not to keep Archie in jail pending trial or let Archie go (perhaps after posting bail<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>)</li>
<li>Court wants Archie to show up and not do any more crimes
<ul>
<li><span class="math inline">\(Y=1\)</span>: Archie will be arrested for another crime if released</li>
<li><span class="math inline">\(Y=0\)</span>: Archie will not be arrested</li>
<li>Similarly for failure to appear on trial date, arrest for violence, etc.</li>
</ul></li>
<li>We’ve worked with data like this before</li>
</ul>
</section>
</section>
<section id="notation" class="level1">
<h1>NOTATION</h1>
<ul>
<li>Archie’s features <span class="math inline">\(=X = (X_p, X_u)\)</span> where <span class="math inline">\(X_p\)</span> are the protected features and <span class="math inline">\(X_u\)</span> are the unprotected ones</li>
<li><span class="math inline">\(Y=\)</span> whether or not Archie will be arrested for another crime before trial
<ul>
<li>Or: will show up for trial, will be re-arrested after being released from prison, will default on the loan, …</li>
<li>Generally, today, <span class="math inline">\(Y=1\)</span> is the bad case</li>
</ul></li>
<li><span class="math inline">\(\hat{Y}(x) =\)</span> prediction we make about someone with features <span class="math inline">\(x\)</span>
<ul>
<li>Here <span class="math inline">\(\hat{Y}=1\)</span> means “we predict re-arrest” (or <strong>recidivism</strong>), and so someone paying attention to use would presumably not release this person</li>
<li><span class="math inline">\(\hat{Y}(x)\)</span> can ignore some features in <span class="math inline">\(x\)</span></li>
</ul></li>
<li><span class="math inline">\(p(x) = \mathbb{P}\left( Y=1|X=x \right)\)</span> is the true risk function
<ul>
<li>Note that true risk function isn’t known</li>
</ul></li>
<li><span class="math inline">\(s(x) =\)</span> risk score we calculate based on <span class="math inline">\(x\)</span>
<ul>
<li>may or may not be an estimate of <span class="math inline">\(p(x)\)</span></li>
</ul></li>
</ul>
</section>
<section id="anti-classification" class="level1">
<h1>“Anti-classification”</h1>
<p><strong>Anti-classification</strong> means: don’t use protected categories to make these decisions/predictions. To make this useful, we’re going to need to be more precise.</p>
<section id="option-i-no-direct-use-of-protected-features" class="level2">
<h2 class="anchored" data-anchor-id="option-i-no-direct-use-of-protected-features">Option I: No direct use of protected features</h2>
<p>In the first formalization, the prediction our system makes must be the same for any two inputs with the <em>same</em> features:</p>
<blockquote class="blockquote">
<p><strong>Not using protected features</strong> means: if we make different predictions for cases <span class="math inline">\(x\)</span> and <span class="math inline">\(x^{\prime}\)</span>, <span class="math inline">\(\hat{Y}(x) \neq \hat{Y}(x^{\prime})\)</span>, then we must have <span class="math inline">\(x_u \neq x_u^{\prime}\)</span>.</p>
</blockquote>
<p>Equivalently, if <span class="math inline">\(x_u = x^{\prime}_u\)</span>, then <span class="math inline">\(\hat{Y}(x) = \hat{Y}(x^{\prime})\)</span>, we can’t make different predictions if two cases <em>just</em> differ in their protected features.</p>
<p>In the risk-prediction context, this would mean that we can’t <em>explicitly</em> look at race, sex, or national origin, say.</p>
<section id="inference-from-unprotected-to-protected-features" class="level3">
<h3 class="anchored" data-anchor-id="inference-from-unprotected-to-protected-features">Inference from unprotected to protected features</h3>
<p>The rule “don’t use the protected features” is often seen as too weak to be satisfying. Sometimes this is for (debated and debatable) ethical reasons<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, but there is also the more technical concern, which is that it is often very easy to infer the values of protected features from the values of unprotected features. Thus for instance knowing <em>where</em> someone lives in is a very good proxy for race (as well as for education and income, which are not, strictly speaking, legally protected features).</p>
<p>As a small example, the dot in the next figure shows the census tract where I grew up, in Montgomery County, Maryland, just outside of Washington DC. (The dot is <em>not</em> on my parents’ house.) The inset table shows some summary demographic information for that tract (from 2019) from the Census Bureau. (It’d also be easy to get information about income, housing conditions, education levels, etc.) Even if we know nothing else about someone, knowing that they’re from census tract 23224-24-031-7044.04 allows us to predict their race with 70% accuracy.</p>
<p><img src="census-tract-bethesda.png" class="img-fluid"></p>
<p><em>Part of the Washington, D.C. metropolitan area, specifically Montgomery County, Maryland. The blue lines are the boundaries between “census tracts”, the geographic area which the Census Bureau uses for data reporting and collection. By design, tracts are supposed to have fairly homogeneous populations of about a few thousand people. The dark dot is in the census tract where I grew up (though not on top of my old house!), and the inset table shows the racial demographics of the tract. Source: [https://geomap.ffiec.gov/FFIECGeocMap/GeocodeMap1.aspx]. (Why does the government agency in charge of maintaining standards for bank examiners have a tool for showing the demographic information of census tracts?)</em></p>
<p>By way of contrast, here’s a census tract not too far away, but in Prince George’s County, Maryland, also just outside Washington, where we could predict whether or not someone was white with over 90% accuracy:</p>
<p><img src="census-tract-pg-cty.png" class="img-fluid"></p>
<p><em>As before, but for a census tract in Prince George’s County, Maryland.</em></p>
<p>The fact that knowing someone’s address — not the exact address, just the zip code or the census tract — tells us a lot about their protected categories should be worrying. It might not seem bad to include zip code as a predictor for criminal risk: crime <em>does</em> cluster spatially, for lots of reasons. But saying “We can safely let Archie go, because people from that part of Bethesda are rarely violent” is very close to saying “We can safely let Archie go, because he’s probably white (or Asian)”.</p>
</section>
<section id="inferring-protected-features-from-web-traffic" class="level3">
<h3 class="anchored" data-anchor-id="inferring-protected-features-from-web-traffic">Inferring protected features from web traffic</h3>
<p>Fine, you say, no zip codes. But you can learn a lot about someone from their browser history, and “On the Internet, nobody knows you’re a dog”, so what’s the problem?</p>
<p><img src="Internet_dog.jpg" class="img-fluid"></p>
<p><em>The classic</em> New Yorker <em>cartoon by Peter Steiner from 1993 <a href="https://en.wikipedia.org/wiki/On_the_Internet,_nobody_knows_you%27re_a_dog">(source)</a>.</em></p>
<p>Only, it turns out, they do.</p>
<p><span class="citation" data-cites="Goel-Hofman-who-does-what-on-the-web">@Goel-Hofman-who-does-what-on-the-web</span> used the full browsing history of about a quarter of a million (US) Web users, primarily to examine how different demographic groups — defined by age, sex, race, education and household income — used the Web differently. If we think of each demographic category as a label <span class="math inline">\(Y\)</span>, and which websites were visited (and how often) as features <span class="math inline">\(X\)</span>, they were primarily interested in <span class="math inline">\(\mathbb{P}\left( X=x|Y=y \right)\)</span>, and how this differed across demographic categories. For instance, people with a post-graduate degree visited news sites about three times as often as people with only a high school degree. (What’s <span class="math inline">\(X\)</span> and what’s <span class="math inline">\(Y\)</span> in that example?) It may or may not surprise you to learn that they found large differences in browsing behavior across demographic groups. To steal an example from the paper, men were much more likely than women to visit ESPN, and women were more likely than men to visit Lancome.</p>
<p>But now this can be turned around. <em>Any</em> feature whose distribution differs between two groups can be used to make a classifier which distinguishes those groups with <em>some</em> accuracy. The more features whose distribution differs, and the more that the distributions differ, the better the classifier can be. This means that someone who knows what websites you browse can predict your age, sex, race, education, and household income. To demonstrate this, <span class="citation" data-cites="Goel-Hofman-who-does-what-on-the-web">@Goel-Hofman-who-does-what-on-the-web</span> used the 10,000 most popular websites, creating a binary feature for each site, <span class="math inline">\(X_i=1\)</span> if site <span class="math inline">\(i\)</span> was visited at all during the study and <span class="math inline">\(X_i=0\)</span> if not. They then used a linear classifier on these features (with a <a href="http://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html">with a geometric margin constraint</a>). The next figure shows how well they were able to predict each of those five demographic variables.</p>
<p><img src="goel-et-al.png" class="img-fluid"></p>
<p><em>Detail of Figure 8 from <span class="citation" data-cites="Goel-Hofman-who-does-what-on-the-web">@Goel-Hofman-who-does-what-on-the-web</span>, showing the ability of a (regularized) linear classifier to predict demographic variables based on web browsing history. Dots show the achieved accuracy, and the <span class="math inline">\(\times\)</span> shows the frequency of the more common class.</em></p>
<p>I include this not because the precise accuracies matter — these aren’t the highest accuracies attainable, even with these features — but rather to make the point that this kind of prediction <em>can</em> be done. It doesn’t matter <em>why</em> different demographic groups have different browsing habits, just <em>that</em> those distinctions make a difference. This lets us (or our machines) work backwards from browsing to accurate-if-imperfect inferences about demographic categories.</p>
<p>Now imagine a recidivism prediction system which does not, officially or explicitly, consider sex, but <em>does</em> have access to the defendant’s web browsing history. (No such system exists, to best of my knowledge, but there’s no intrinsic limit on its creation<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.) We just saw that sex can be predicted with (at least) 80% accuracy from browsing history. A nefarious designer who wanted to include sex as a predictor for recidivism, but also wanted to hide doing so, could therefore use browsing history to predict sex, and then include predicted sex in their model. A less nefarious designer might end up doing something equivalent without even realizing it, say by slightly increasing the predicted risk of those who visit ESPN and slightly reducing the prediction for those who visit Lancome, and so on down all the websites whose popularity predicts sex. Either designer might, when pressed, say that they’re not claiming to say <em>why</em> ESPN predicts recidivism, but facts and facts, and are you going to argue with the math?</p>
<p>In fact, we can go further. We know that younger people have a higher risk of violence than older people, that poorer people have a higher risk than richer people, that men have a higher risk than women, that blacks have a higher risk than whites<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, that less educated people have a higher risk than more educated people<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="citation" data-cites="Shadows-of-doubt">[@Shadows-of-doubt]</span>. A system which just used Web browsing to sort people on those five attributes could<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, therefore, achieve non-trivial predictive power. You can even imagine designing such a system innocently, where we just try to boil down a large number of features into (say) a five-dimensional space, before using them to predict violence, without realizing that those five dimensions correspond to age, sex, race, income and education.</p>
<p>None of this really relies on the features being Web browsing history; anything whose distribution differs across demographic groups will do.</p>
</section>
</section>
<section id="option-ii-decisions-must-be-independent-of-protected-features" class="level2">
<h2 class="anchored" data-anchor-id="option-ii-decisions-must-be-independent-of-protected-features">Option II: Decisions must be <em>independent</em> of protected features</h2>
<p>All of this motivates a stronger notion of anti-classification: not only should you not <em>explicitly</em> use the protected features, you shouldn’t sneak them in the back via inference from the unprotected features. The cleanest formulation of this I know of comes from <span class="citation" data-cites="DeDeo-wrong-side-of-the-tracks">@DeDeo-wrong-side-of-the-tracks</span>, from our Department of Social and Decision Sciences, which is this:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\hat{Y}(X)\)</span> should be statistically independent of <span class="math inline">\(X_p\)</span>.</p>
</blockquote>
<p><span class="citation" data-cites="DeDeo-wrong-side-of-the-tracks">@DeDeo-wrong-side-of-the-tracks</span> shows that one way to achieve this is to deliberately distort the distribution of the features. Specifically, instead of the actual joint distribution</p>
<p><span class="math display">\[
\mathbb{P}\left( Y=y, X_u = x_u, X_p = x_p \right)
\]</span></p>
<p>you should use the distorted distribution</p>
<p><span class="math display">\[
\tilde{P}(y, x_u, x_p) = \mathbb{P}\left( Y=y, X_u=x_u, X_p=x_p \right)\frac{\mathbb{P}\left( Y=y \right)}{\mathbb{P}\left( Y=y|X_p=x_p \right)}
\]</span></p>
<p>This particular distribution is the closest one, information-theoretically, to the original <span class="math inline">\(\mathbb{P}\left( Y, X_u, X_p \right)\)</span> in which the desired independence still holds. Any other distribution where the independence holds is further from the actual distribution, and more easily distinguished from it by a statistical test.</p>
<p>(In practice, though <span class="citation" data-cites="DeDeo-wrong-side-of-the-tracks">@DeDeo-wrong-side-of-the-tracks</span> doesn’t say this, one way to achieve the distortion would be to fit your model after <em>weighting</em> the data points. The weight of data point <span class="math inline">\(i\)</span> would be</p>
<p><span class="math display">\[
\frac{\mathbb{P}\left( Y=y_i \right)}{\mathbb{P}\left( Y_i=y_i|X_p={x_p}_{i} \right)}
\]</span></p>
<p>so we give more weight to data points with labels which their protected attributes make relatively unlikely. Since we don’t know the true probabilities, we’d have to estimate them.)</p>
<p>This is an elegant solution but I don’t think anyone except Simon uses it.</p>
</section>
</section>
<section id="classification-parity" class="level1">
<h1>“Classification parity”</h1>
<p>When people think about fairness as <strong>parity</strong>, they mean that rates, or error rates, should be equal across groups defined by protected attributes. There are four main versions of this.</p>
<section id="demographic-parity" class="level2">
<h2 class="anchored" data-anchor-id="demographic-parity">Demographic Parity</h2>
<blockquote class="blockquote">
<p><strong>Demographic parity</strong>: <span class="math inline">\(\mathbb{P}\left( \hat{Y}(X) = 1| X_p \right) = \mathbb{P}\left( \hat{Y}(X)=1 \right)\)</span></p>
</blockquote>
<p>In the recidivism-prediction context, this would mean that we should have equal rates of detention (or release) across groups.</p>
<p>Violations of demographic parity are basically the same as what US law calls “disparate impact”. This is not <em>necessarily</em> illegal, but it’s the kind of thing which needs to be justified by “business necessity”, i.e., showing that it’s really essential to the goal the organization is trying to achieve. Even then, courts or regulators might ask whether there isn’t another way of getting the job done with less disparate impact.</p>
<p>(Thought exercise: Does demographic parity imply that <span class="math inline">\(\hat{Y}\perp X_p\)</span>?)</p>
<p>Note that implementing demographic parity will often require using <em>different</em> thresholds on <span class="math inline">\(p(x)\)</span> for each group. As we discussed in class, this creates a tension between disparate impact and disparate treatment.</p>
<p>(The Supreme Court case I mentioned in class, where the New Haven fire department seemed to be put in a double bind of committing one or the other, was <em>Ricci vs.&nbsp;DeStefano</em> of 2009; there is an interesting and thorough discussion in <span class="citation" data-cites="RTFord-gone-wrong">@RTFord-gone-wrong</span>, pp.&nbsp;107–127.)</p>
</section>
<section id="error-rate-parities" class="level2">
<h2 class="anchored" data-anchor-id="error-rate-parities">Error-Rate Parities</h2>
<section id="false-positive-parity" class="level3">
<h3 class="anchored" data-anchor-id="false-positive-parity">False-Positive Parity</h3>
<blockquote class="blockquote">
<p><strong>FPR parity</strong>: equal false positive rates across groups,</p>
</blockquote>
<p><span class="math display">\[
\mathbb{P}\left( \hat{Y}(X)=1|Y=0, X_p \right) = \mathbb{P}\left( \hat{Y}(X)=1|Y=0 \right)
\]</span></p>
<p>In the context of pre-trial detention, this would mean equal detention rates among those who would <em>not</em> have commited a crime if released.</p>
<p>It may (as in that example) be very hard to know what those rates are, a point we’ll return to below.</p>
</section>
<section id="false-negative-partiy" class="level3">
<h3 class="anchored" data-anchor-id="false-negative-partiy">False-Negative Partiy</h3>
<blockquote class="blockquote">
<p><strong>FNR parity</strong>: equal false negative rates across groups,</p>
</blockquote>
<p><span class="math display">\[
\mathbb{P}\left( \hat{Y}(X)=0|Y=1, X_p \right) = \mathbb{P}\left( \hat{Y}(X)=0|Y=1 \right)
\]</span></p>
<p>Concretely: Equal probability of detention among those who would have gone on to commit a crime had they been released</p>
</section>
</section>
<section id="predictive-value-parity" class="level2">
<h2 class="anchored" data-anchor-id="predictive-value-parity">Predictive-Value Parity</h2>
<blockquote class="blockquote">
<p><strong>PPV/NPV parity</strong>: equal positive and negative predictive values across groups,</p>
</blockquote>
<p><span class="math display">\[
\mathbb{P}\left( Y=1|\hat{Y}(X), X_p \right) = \mathbb{P}\left( Y=1|\hat{Y}(X) \right)
\]</span></p>
<p>so outcome is independent of protected attributes <em>given</em> the prediction.</p>
</section>
<section id="measuring-parity-violations" class="level2">
<h2 class="anchored" data-anchor-id="measuring-parity-violations">Measuring Parity Violations</h2>
<p>Because all these different forms of parity say that various rates should be <em>equal</em>, there’s an easy and natural way to see how badly a system violates parity: take the difference in rates. Thus we might define</p>
<p><span class="math display">\[
\Delta_{FNR} \equiv |\mathbb{P}\left( \hat{Y}(X)=0|Y=1, X_p=1 \right) - \mathbb{P}\left( \hat{Y}(X)=0|Y=1, X_p=0 \right)|
\]</span></p>
<p>(If there are more than two groups, we could take the sum of all the pairwise differences, or the maximum pairwise difference, etc.)</p>
<p>Of course, the difference in rates isn’t the only way to go. We could also look at the <em>ratio</em> of rates, and how far that is from 1. In the US, the administrative agencies which enforce anti-disrimination laws generally aren’t interested in disparate impact claims until the success rate for one group drops below 80% of the rate from another; this is a ratio-of-rates standard rather than a difference-in-rates standard.</p>
</section>
</section>
<section id="calibration" class="level1">
<h1>Calibration</h1>
<blockquote class="blockquote">
<p>A risk score <span class="math inline">\(s(X)\)</span> is <strong>calibrated</strong>, or <strong>equally calibrated</strong>, when</p>
</blockquote>
<p><span class="math display">\[
\mathbb{P}\left( Y=1|s(X), X_p \right) = \mathbb{P}\left( Y=1|s(X) \right)
\]</span></p>
<p>Calibration is equivalent to saying that</p>
<p><span class="math display">\[
Y \perp X_p | s(X)
\]</span></p>
<p>(Can you show this is equivalent?) Notice that this <em>must</em> be true if <span class="math inline">\(s(X) = p(x) \equiv \mathbb{P}\left( Y=1|X=x \right)\)</span>.</p>
<section id="calibration-almost-implies-ppv-parity" class="level2">
<h2 class="anchored" data-anchor-id="calibration-almost-implies-ppv-parity">Calibration <em>almost</em> implies PPV parity</h2>
<p>Suppose our score is calibrated. We apply a threshold <span class="math inline">\(t\)</span>, and set <span class="math inline">\(\hat{Y}=1\)</span> if <span class="math inline">\(s \geq t\)</span> and <span class="math inline">\(\hat{Y}=0\)</span> otherwise. The positive predictive value is <span class="math display">\[\begin{eqnarray}
\mathbb{P}\left( Y=1|\hat{Y}=1 \right) &amp; = &amp; \sum_{s}{\mathbb{P}\left( Y=1|\hat{Y}=1, S=s \right)\mathbb{P}\left( S=s|\hat{Y}=1 \right)}\\
&amp; = &amp; \sum_{s\geq t}{\mathbb{P}\left( Y=1|\hat{Y}=1, S=s \right)\mathbb{P}\left( S=s|S \geq t \right)}\\
&amp; = &amp; \sum_{s\geq t}{\mathbb{P}\left( Y=1|S=s \right)\mathbb{P}\left( S=s|S \geq t \right)}
\end{eqnarray}\]</span> using the law of total probability, and the way <span class="math inline">\(\hat{Y}\)</span> is defined in terms of <span class="math inline">\(S\)</span>. Now if we condition on the protected attributes <span class="math inline">\(X_p\)</span>,</p>
<p><span class="math display">\[
\begin{eqnarray}
\mathbb{P}\left( Y=1|\hat{Y}=1, X_p \right) &amp;= &amp; \sum_{s\geq t}{\mathbb{P}\left( Y=1|S=s, X_p \right)\mathbb{P}\left( S=s|S \geq t, X_p \right)}\\
&amp; = &amp; \sum_{s\geq t}{\mathbb{P}\left( Y=1|S=s \right)\mathbb{P}\left( S=s|S \geq t, X_p \right)}
\end{eqnarray}
\]</span></p>
<p>using the assumption that the risk score is calibrated. So if the risk score is calibrated, and the <em>distribution</em> of above-threshold risk scores is the same across groups, we’ll get equal PPV.</p>
<p>Going the other way, if we start from the assumption of equal PPV across groups, then we have a really big coincidence <em>unless</em> the risk score is calibrated.</p>
</section>
</section>
<section id="tensions" class="level1">
<h1>Tensions</h1>
<section id="accuracy-vs.-fairness" class="level2">
<h2 class="anchored" data-anchor-id="accuracy-vs.-fairness">Accuracy vs.&nbsp;Fairness</h2>
<p>In general, we build our classifiers (or other predictive system) to maximize some notion of accuracy, or minimize some loss function (like the log-probability loss). To be brief, I’ll just talk about “accuracy”. Accuracy is not any of these notions of fairness, and it doesn’t <em>imply</em> any of them either. If we build a machine to maximize accuracy, we should not be surprised when it does not also do other things, no matter how desirable those might be, or how much common sense might say they matter too.</p>
<p>Of course, “maximize accuracy” is not a command handed to us from Above, it’s a choice we make, and we could imagine replacing it with other goals. For instance, we might say “maximize fairness”, which more mathematically and operationally might mean “minimize the violation of false positive rate parity”. But there are dumb, perverse ways of achieving this goal, too. For instance, if a judge rolled a 20 sided die and let everyone go unless he rolled a 1, the false positive rate would be 5% for everyone. It would also have demographic parity and false negative rate parity. But most of us would regard this as unacceptable, partly because it’s so obviously random, but still more because that equalized false negative rate would be 95%! Matters would not be any better if, instead of judges rolling weird dice, we got the same result by feeding defendants’ features through a complicated piece of code that spat out effectively-random numbers<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p>The sensible thing to do is to admit that accuracy and fairness both matter, and to look at the trade-off between them. Concretely, this will often amount to making a plot of accuracy against (say) the difference in FPRs we get from different ways of making predictions — different methods, or different thresholds within one method, or both. Sometimes, one option will do better on <em>both</em> scores than another — it’ll be more accurate <em>and</em> more fair. Decision-theorists would say that the more-accurate-and-more-fair option <strong>dominates</strong> the less-accurate-and-less-fair option, and we should “eliminate dominated alternatives”. But once we’ve eliminated the dominated alternatives, we’ll be left with choosing between more accuracy and less fairness, or less fairness and more accuracy. That sounds pretty politically and ethically fraught, because it is. You might be tempted to chose <em>weights</em> for accuracy and fairness, and just maximize their sum; you’ll then pick one of the un-dominated options, but which one you pick will depend on the weight you chose. So choosing weights is just as politically and ethically fraught as directly choosing among the un-dominated options. (Conversely, any choice among the un-dominated options is equivalent to choosing weights for accuracy and fairness.)</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="lecture-24_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><em>Example of what a plot of classification accuracy against the violation of FPR parity might look like. (The precise numbers here are made up, but not too dissimilar from something you’ll see in the homework.) We can achieve combinations of accuracy and parity which lie above the curve, but not below.</em></p>
</section>
<section id="fairness-vs.-fairness" class="level2">
<h2 class="anchored" data-anchor-id="fairness-vs.-fairness">Fairness vs.&nbsp;Fairness</h2>
<p>In general, none of these three forms of fairness — anti-discrimination, parity of rates, and calibration — implies the other. In fact, there is a fundamental problem with trying to achieve both parity and calibration:</p>
<blockquote class="blockquote">
<p>If true rates of recidivism are different by group, then you cannot have calibration <em>and</em> equal error rates <span class="citation" data-cites="Chouldechova-fair-prediction-disparate-impact">[@Chouldechova-fair-prediction-disparate-impact]</span>.</p>
</blockquote>
<p>Specifically, <span class="citation" data-cites="Chouldechova-fair-prediction-disparate-impact">@Chouldechova-fair-prediction-disparate-impact</span> claims “it is straightforward to show that”</p>
<p><span class="math display">\[
FPR = \frac{R}{1-R}\frac{1-PPV}{PPV}(1-FNR)
\]</span></p>
<p>where <span class="math inline">\(R = \mathbb{P}\left( Y=1 \right)\)</span> (see backup). This equation holds both for the overall population, and separately for each group.</p>
<p>Now suppose that <span class="math inline">\(R\)</span> is different for each group.</p>
<ul>
<li>If we’re calibrated, we’ll have equal PPV for each group, so (because <span class="math inline">\(R\)</span> is different for each group) we <em>must</em> have different FPR and/or FNR by groups, and that means we violate parity of error rates.</li>
<li>If we have equal error rates across groups, then PPV must be different across groups, and we can’t be calibrated.</li>
</ul>
<p>Finally, if PPV and error rates are the same across groups, then prevalence <span class="math inline">\(R\)</span> must be equal across groups as well.</p>
<p>To sum up, we can’t be <em>both</em> calibrated <em>and</em> have equal error rates.</p>
</section>
</section>
<section id="the-lurking-problem-designedly-missing-data" class="level1">
<h1>The lurking problem: designedly missing data</h1>
<p>Suppose we, the legal system, hold everyone with <span class="math inline">\(\hat{Y}=1\)</span> until trial, and only release those with <span class="math inline">\(\hat{Y}=0\)</span>. Then we have no <em>data</em> about <span class="math inline">\(\mathbb{P}\left( Y=0|\hat{Y}=1 \right)\)</span>, since we’ve made sure that can’t happen. We do however get to see <span class="math inline">\(\mathbb{P}\left( Y=0|\hat{Y}=0 \right)\)</span>, i.e., the negative predictive value. (Notice by the way that every case contributing to <span class="math inline">\(\mathbb{P}\left( Y=1|\hat{Y}=1 \right)\)</span> is apt to make our jobs uncomfortable: “why did you let <em>them</em> out?”) Similarly, it’s hard for lenders to know how many borrowers they rejected would have paid back their loans, or for colleges to know how many rejected applicants would have done well at their school, or for employers to know how many rejected job applicants would have been good workers.</p>
<p>(Lenders or schools might try to get around this by seeing if the rejected applicants got loans from other lenders, or went to other schools, but issues of proxy quality will still arise [was that school <em>really</em> similar?] It takes a very unusual situation for this to be an option for the courts.)</p>
<p>Historical data, from before the prediction system, doesn’t really get around this. Historical data on recidivism is only available for those released by the courts , which introduces all sorts of weird biases. E.g., suppose having a history of being a gang member usually ruled out pre-trial release, and all the exceptions who were released were really unusual people who can (say) prove to the courts that they’ve totally turned around their lives. Then, <em>in the historical data</em>, gang membership could well be associated with <em>lower</em> risk of recidivism.</p>
</section>
<section id="algorithmic-fairness-isnt-really-about-algorithms" class="level1">
<h1>Algorithmic fairness isn’t really about <em>algorithms</em></h1>
<p>You may have noticed that I haven’t had to say anything about data processing or computer code. That’s because all these notions of “algorithmic fairness”, so called, are not, in fact, about algorithms at all. They’re about decision-makers, or decision-making systems. A human judge, or a human loan officer, or a corporate hiring process made up entirely of human beings, will make some correct decisions and some incorrect ones, and it’s just as sensible to talk about their error rates, calibration, etc. If we’re concerned with whether introducing an algorithm to decide who’s safe to release before trial is being unfair (perhaps by making too many false positives for some groups), it seems legitimate to ask whether it’s <em>less</em> unfair than the human beings it is replacing<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. But we typically don’t <em>know</em> the error rates of human judges. If we’re really concerned with these formalizations of “fairness”, though, that would seem to be an important question.</p>
</section>
<section id="summing-up" class="level1">
<h1>Summing up</h1>
<ul>
<li>Basic anti-classification (don’t use protected attributes) is easy, but leaves open proxies</li>
<li>Classification parity is a solvable technical problem</li>
<li>Calibration is also a solvable technical problem</li>
<li>We cannot possibly achieve all three of anti-classification, classification parity, and calibration.
<ul>
<li>We can’t even really achieve both classification parity and calibration.</li>
</ul></li>
<li>For many applications, actually following our predictions would remove the data needed to see whether we were right or not</li>
</ul>
<section id="backup-filling-in-chouldechovas-it-is-straightforward-to-show-that" class="level2">
<h2 class="anchored" data-anchor-id="backup-filling-in-chouldechovas-it-is-straightforward-to-show-that">Backup: Filling in Chouldechova’s “it is straightforward to show that”</h2>
<p>(maybe it’s straightforward for <em>Alex</em>…)</p>
<p>I’ll write out the algebra for the population as a whole; doing it for each group just means sprinkling in conditioning signs.</p>
<p><span class="math inline">\(R = \mathbb{P}\left( Y=1 \right)\)</span> is the true prevalence or base rate.</p>
<p>Chouldechova’s claim is that [ FPR = (1-FNR) ]</p>
<p>Substituting in from the definitions, [ = (1-) ] Since <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> are both binary, [ = ] but <span class="math display">\[\begin{eqnarray}
\mathbb{P}\left( Y=0|\hat{Y}=1 \right) &amp; = &amp; \mathbb{P}\left( Y=0, \hat{Y}=1 \right)/\mathbb{P}\left( \hat{Y}=1 \right)\\
&amp;= &amp; \mathbb{P}\left( \hat{Y}=1|Y=0 \right)\mathbb{P}\left( Y=0 \right)/\mathbb{P}\left( \hat{Y}=1 \right)\\
\mathbb{P}\left( Y=1|\hat{Y}=1 \right) &amp; = &amp; \mathbb{P}\left( \hat{Y}=1|Y=1 \right)\mathbb{P}\left( Y=1 \right)/\mathbb{P}\left( \hat{Y}=1 \right)
\end{eqnarray}\]</span> so <span class="math display">\[\begin{eqnarray}
\mathbb{P}\left( Y=0|\hat{Y}=1 \right) / \mathbb{P}\left( Y=1|\hat{Y}=1 \right) &amp; = &amp; \mathbb{P}\left( \hat{Y}=1|Y=0 \right)\mathbb{P}\left( Y=0 \right) / \mathbb{P}\left( \hat{Y}=1|Y=1 \right) \mathbb{P}\left( Y=1 \right)\\
\frac{\mathbb{P}\left( Y=1 \right)}{\mathbb{P}\left( Y=0 \right)} \frac{\mathbb{P}\left( Y=0|\hat{Y}=1 \right)}{ \mathbb{P}\left( Y=1|\hat{Y}=1 \right)} &amp; = &amp;\mathbb{P}\left( \hat{Y}=1|Y=0 \right) / \mathbb{P}\left( \hat{Y}=1|Y=1 \right)
\end{eqnarray}\]</span> and so, substituting in, we get [ = ] which is certainly true.</p>
</section>
</section>
<section id="further-reading" class="level1">
<h1>Further Reading</h1>
<p>My presentation of this topic largely (but not entirely) follows the excellent review paper by <span class="citation" data-cites="Corbett-Davies-and-Goel-mismeasure-of-fairness">@Corbett-Davies-and-Goel-mismeasure-of-fairness</span>. On accuracy-fairness and fairness-fairness trade-offs, a good introduction is <span class="citation" data-cites="Kearns-Roth-ethical">@Kearns-Roth-ethical</span>.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Cash bail is when someone who’s been arrested can leave jail, until their trial, <em>if</em> they deposit money with the court to show they’re serious (“post bail”); the money gets returned to them if they do, in fact, show up for their trial. Typically arrestees (or their friends and family) don’t do this directly, they go to an agent called “bail bondsman” and pay the bondsman a fraction of the official bail, maybe 10%, and the bondsman posts the bail with the court. If the arrestee shows up for trial, the bondsman gets the whole bond amount from the court, and takes the 10% (or whatever) as his fee; if the arrestee doesn’t show up, the bondsman is out the whole amount, and will often try to locate the arrestee and force them to go to court.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The basic thought is that members of some groups have suffered from a history of injustices and unfairness, or are suffering from injustice and unfairness in other areas of life right now, then <em>this</em> system, which we’re designing, ought to take steps to do what it can to make that better. This is not obivously right, but it’s also an idea which has appealed to many people, not just the ones who would immediately benefit from it.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Systems like this are currently used all the time for making consumer credit decisions, as discussed in <span class="citation" data-cites="ONeil-WMD">@ONeil-WMD</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>There are multiple reasons for this association. One is a long-standing history (cf. <span class="citation" data-cites="Dollard-on-Southerntown">@Dollard-on-Southerntown</span>) of segregating African-Americans into neighborhoods which are under-policed (in the sense that violence often goes unpunished by the forces of the law) and over-policed (in the sense that interactions with the police are often hostile). This sets up a dynamic where people in those neighborhoods don’t trust the police, which makes the police ineffective, which makes being known for willingness to use violence a survival strategy, which etc., etc. <span class="citation" data-cites="Leovy-ghettoside">@Leovy-ghettoside</span> gives a good account of this feedback loop from (mostly) the side of the police; <span class="citation" data-cites="Allen-cuz">@Allen-cuz</span> gives a glimpse of what it looks like from the other side. For further details, see <span class="citation" data-cites="Shadows-of-doubt">[@Shadows-of-doubt]</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Cathy O’Neil would remind us that many of these would flip around if we considered risk of <em>financial</em> crimes rather than violence <span class="citation" data-cites="ONeil-WMD">[@ONeil-WMD]</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>I say “could”, because there’s some error in all these classifications, and it’s <em>possible</em> that these errors would cancel out the ability to predict violence from demographics.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Take all the features, concatenate them into one big number, and make that the seed in your random number generator…<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Strictly speaking, judges aren’t replaced by algorithms in pre-trial release decisions, at least not in the US, they’re just given risk scores from statistical models. How much that influences the judges’ decisions is an interesting and under-explored question. In some other fields, like a lot of consumer loan decisions, the model’s output rules.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>