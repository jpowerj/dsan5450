[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to DSAN 5450: Data Ethics and Policy at Georgetown University!\nThe course meets on Wednesdays from 3:30-6pm in the Walsh Building, Room 498"
  },
  {
    "objectID": "syllabus.html#course-staff",
    "href": "syllabus.html#course-staff",
    "title": "Syllabus",
    "section": "Course Staff",
    "text": "Course Staff\n\nProf. Jeff Jacobs, jj1088@georgetown.edu\n\nOffice hours: TBD\n\nTA 1\n\nOffice hours: TBD\n\nTA 2\n\nOffice hours: TBD"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis graduate-level course will train students to navigate the landscape of ethical issues which arise at each step of the data science process, with an eye towards developing policy recommendations for governments and organizations seeking expert advice on how to tackle these issues from a regulatory perspective. Students will explore and critically evaluate a range of data-related issues in contemporary society, such as responsible data collection, algorithmic bias, privacy, transparency, accountability, democratic participation in data usage and data-driven decisions, and the ethical implications of emerging technologies like artificial intelligence and machine learning (self-driving cars, ChatGPT, crowd-sourced training data, etc.).\nBeginning with a set of historical case studies—instances in which scientists, engineers, and policymakers have been forced to re-evaluate their ethical intuitions in light of technological developments (nuclear power, use of social media platforms to organize protests and influence political outcomes, deployment of facial recognition software and predictive AI by police and military forces)—the course then introduces a set of general ethical frameworks (consequentialism, deontological ethics, and virtue ethics), challenging students to consider their relative strengths and weaknesses for addressing modern technological-ethical dilemmas faced by businesses, healthcare organizations, governments, and academic institutions. After a final portion of the course linking these ethical frameworks with practical regulatory and policy considerations, students will write and present a policy whitepaper analyzing a data-ethical issue of particular interest to them, integrating ethical perspectives, regulatory principles, and domain knowledge into a recommendation of best practices for the relevant agency, firm, or institution.\nThe course will thus equip students with a robust ethical “toolbox” for conscientiously gathering, interpreting, and extracting meaning from data throughout their careers as data scientists, while respecting privacy, fairness, transparency, democratic accountability, and other social concerns. Prerequisites: None. 3 credits."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nThe course revolves around three “pillars”, which we’ll examine individually before bringing them together for your final projects at the end of the class.\n\nData Science\nA portion of the course will focus on introductions to cutting-edge technologies like self-driving cars, ChatGPT, facial detection algorithms, and various applications of AI to police and military technologies. For this portion, we’ll draw fairly often from the contents of the following books:\n\nCatherine D’Ignazio and Lauren F. Klein (2020). Data Feminism. Cambridge, MA: MIT Press. [Free, open-source!]\nCathy O’Neil (2016). Weapons of Math Destruction. New York, NY: Crown Books.\n\nSince there are plenty of in-depth resources available to you (e.g., other Georgetown courses!) for learning the technical details of these technologies, our goal in this course will be to learn just the particular aspects of each technology which are most relevant to the ethical and policy issues they present.\nFor example, we will look at Neural Netwok-based Machine Learning algorithms, but we will focus specifically on how the performance of these algorithms on a given task depends crucially on the existence of effective training data for that task. The breakthroughs in Artificial Intelligence which have had an immense impact on society over the past few decades, for example, have not come about because of new algorithms (neural networks, for example, have been around since the 1950s). Rather, they have come about because of the massive, exponential increase in the amount of data available to train these already-existing algorithms: for example, data scraped from across the entire web, or from millions of scanned books, or from Wikipedia’s massive collection of articles. This means, therefore, that these algorithms simply encode pre-existing human biases into algorithmically-derived “rules”, thus motivating the next pillar of the course: Ethics!\n\n\nEthics\nFor the ethics-focused portion of the course, we’ll be reading selections from the following textbook:\n\nLewis Vaughn and Louis P. Pojman (2021). The Moral Life: An Introductory Reader in Ethics and Literature. Oxford, UK: Oxford University Press. [PDF]\n\nFrom the vast array of readings contained in this collection, we’ll look at both “standard” ethical readings from e.g. Jeremy Bentham and Immanuel Kant plus readings from literary sources like Ursula Le Guin and Ambrose Bierce.\n\n\nPublic Policy\nFor the final piece of the course we will take the technological developments discussed the first portion, analyze them using the ethical frameworks discussed in the second portion, and come to conclusions as to what types of things lawmakers, governments, and civil society organizations (NGOs, for example, and Think Tanks) can do in practice to address the ethical issues raised by these technologies. This means that, specifically, the recommended final project for the course will be a Policy Whitepaper, where you will choose a particular institution and make a recommendation to them in terms of how they can use their power (for example, the power to pass laws) to most effectively address an ethical issue that you believe is important.\nFor this portion of the class we’ll have to draw on a wide range of different readings, depending on what particular subdomains of public policy are most interesting to you all, but as a general textbook on ethics in data science which does focus a good amount on policy specifically, we will look at:\n\nAnne L. Washington (2023). Ethical Data Science: Prediction in the Public Interest. New York, NY: Oxford University Press.\n\nNow that you have an overview of the trajectory of the course, the following section contains the particulars of what we’ll be reading and working on each week!"
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe following is a rough map of what we will work through together throughout the semester; given that everyone learns at a different pace, my aim is to leave us with a good amount of flexibility in terms of how much time we spend on each topic: if I find that it takes me longer than a week to convey a certain topic in sufficient depth, for example, then I view it as a strength rather than a weakness of the course that we can then rearrange the calendar below by adding an extra week on that particular topic! Similarly, if it seems like I am spending too much time on a topic, to the point that students seem bored or impatient to move onto the next topic, we can move a topic intended for the next week to the current week!\n\n\n\nUnit\nWeek\nDate\nTopic\n\n\n\n\nUnit 1: Data-Scientific Issues\n1\nJan 17\nCourse Intro\n\n\n\n2\nJan 24\nMachine Learning and Training Data\n\n\n\n3\nJan 31\nCase Studies Part 1\n\n\n\n4\nFeb 7\nCase Studies Part 2\n\n\n\n\nFeb 9 (Friday)\nDeliverable: Data Science\n\n\nUnit 2: Ethical Frameworks\n5\nFeb 14\nConsequentialism\n\n\n\n6\nFeb 21\nDeontological Ethics\n\n\n\n\nFeb 23 (Friday)\nDeliverable: Ethical Frameworks\n\n\n\n7\nFeb 28\nMidterm (Data Science and Ethics)\n\n\n\n\nMar 6\nNo Class (Spring Break)\n\n\nUnit 3: Applying Ethical Frameworks\n8\nMar 13\nApplying Ethical Frameworks: Self-Driving Cars, Facial Recognition, and ChatGPT\n\n\n\n9\nMar 20\nApplying Ethical Frameworks: Police and Military Applications of AI\n\n\n\n\nMar 22 (Friday)\nDeliverable: Applying Ethical Frameworks\n\n\nUnit 4: Public Policy\n10\nMar 27\nIntro to Public Policy\n\n\n\n11\nApr 3\nAuthoring Policy Whitepapers\n\n\n\n\nApr 5 (Friday)\nDeliverable: Public Policy\n\n\n\n12\nApr 10\nApplications: Public Policy and Climate Justice\n\n\n\n13\nApr 17\nApplications: Race, Class, Gender, Sexuality, and Disability (Data Feminism)\n\n\n\n14\nApril 24\nApplications: Public Policy and International Law\n\n\n\n\nMay 3 (Friday)\nDeliverable: Policy Whitepaper"
  },
  {
    "objectID": "syllabus.html#assignments-and-grading",
    "href": "syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and Grading",
    "text": "Assignments and Grading\nThe main assignment in the course will be your policy whitepaper, submitted at the end of the semester. However, there will also be a midterm exam and a series of assignments which exist to let you explore each of the modules of the course, in turn.\n\n\n\n\n\n\n\n\nAssignment\nDue Date\n% of Grade\n\n\n\n\nData Science Overview Assignment\nFriday, February 9\n10%\n\n\nEthical Frameworks Assignment\nFriday, February 23\n10%\n\n\nMidterm\nWednesday, February 28\n30%\n\n\nApplying Ethical Frameworks Assignment\nFriday, March 22\n10%\n\n\nPublic Policy Assignment\nFriday, April 5\n10%\n\n\nPolicy Whitepaper\nFriday, May 3\n30%"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5450: Data Ethics and Policy",
    "section": "",
    "text": "Welcome to the homepage for DSAN 5450: Data Ethics and Policy at Georgetown University, for the Spring 2024 semester!\nThe course meets on Wednesdays from 3:30pm to 6:00pm in the Walsh Building, Room 498.\nCheck out the syllabus (or any other link in the sidebar) for more info! Or, use the following links to view notes for individual weeks:\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Introduction to the Course\n\n\nJanuary 17\n\n\n\n\nWeek 2: Machine Learning and Training Data\n\n\nJanuary 24\n\n\n\n\nWeek 3: Ethical Frameworks\n\n\nJanuary 30\n\n\n\n\nWeek 4: Fairness in AI\n\n\nFebruary 6\n\n\n\n\nWeek 5: Anonymity, Privacy-Preserving Computation\n\n\nFebruary 13\n\n\n\n\n\n\nNo matching items\n\n\nCourse Description:\nThis graduate-level course will train students to navigate the landscape of ethical issues which arise at each step of the data science process, with an eye towards developing policy recommendations for governments and organizations seeking expert advice on how to tackle these issues from a regulatory perspective. Students will explore and critically evaluate a range of data-related issues in contemporary society, such as responsible data collection, algorithmic bias, privacy, transparency, accountability, democratic participation in data usage and data-driven decisions, and the ethical implications of emerging technologies like artificial intelligence and machine learning (self-driving cars, ChatGPT, crowd-sourced training data, etc.).\nBeginning with a set of historical case studies—instances in which scientists, engineers, and policymakers have been forced to re-evaluate their ethical intuitions in light of technological developments (nuclear power, use of social media platforms to organize protests and influence political outcomes, deployment of facial recognition software and predictive AI by police and military forces)—the course then introduces a set of general ethical frameworks (consequentialism, deontological ethics, and virtue ethics), challenging students to consider their relative strengths and weaknesses for addressing modern technological-ethical dilemmas faced by businesses, healthcare organizations, governments, and academic institutions. After a final portion of the course linking these ethical frameworks with practical regulatory and policy considerations, students will write and present a policy whitepaper analyzing a data-ethical issue of particular interest to them, integrating ethical perspectives, regulatory principles, and domain knowledge into a recommendation of best practices for the relevant agency, firm, or institution.\nThe course will thus equip students with a robust ethical “toolbox” for conscientiously gathering, interpreting, and extracting meaning from data throughout their careers as data scientists, while respecting privacy, fairness, transparency, democratic accountability, and other social concerns. Prerequisites: None. 3 credits."
  },
  {
    "objectID": "w05/slides.html#the-right-to-privacy",
    "href": "w05/slides.html#the-right-to-privacy",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "The “Right” to Privacy",
    "text": "The “Right” to Privacy\n\nIn Theory (Normatively): A right is an individual “veto” on a collective decision (Dworkin 1977)\nIn Practice (Descriptively): Rights grow out of the barrel of a gun"
  },
  {
    "objectID": "w05/slides.html#all-hope-is-not-lost",
    "href": "w05/slides.html#all-hope-is-not-lost",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "All Hope Is Not Lost!",
    "text": "All Hope Is Not Lost!\n\nAnonymity via Synthetic Datasets: if identifying info already exists (e.g., in the US Census), we can generate synthetic datasets which\n\nHave the same distributional properties as the real data, but\nCannot be used to identify you\n\n\\(\\varepsilon\\)-Differential Privacy: Allows us to quantify the tradeoff between accuracy and privacy\nHomomorphic Cryptography makes it possible to encode data in such a way that:\n\nIt can be used to train algorithms, yet\nIt cannot be used to identify you"
  },
  {
    "objectID": "w05/slides.html#the-goal",
    "href": "w05/slides.html#the-goal",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "The Goal",
    "text": "The Goal\n\nEnable mathematical analysis of a privacy mechanism, via optimization with respect to the accuracy-privacy tradeoff:"
  },
  {
    "objectID": "w05/slides.html#pareto-improvements-and-the-pareto-frontier",
    "href": "w05/slides.html#pareto-improvements-and-the-pareto-frontier",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Pareto Improvements and the Pareto Frontier",
    "text": "Pareto Improvements and the Pareto Frontier"
  },
  {
    "objectID": "w05/slides.html#varepsilon-differential-privacy-definition",
    "href": "w05/slides.html#varepsilon-differential-privacy-definition",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "\\(\\varepsilon\\)-Differential Privacy Definition",
    "text": "\\(\\varepsilon\\)-Differential Privacy Definition\n\nA privacy mechanism \\(M\\) provides \\(\\varepsilon\\)-differential privacy if:\n\nFor all pairs of datasets \\(x\\) and \\(y\\) which differ in the data of one person, and\nFor all (probability-theoretic) events \\(S\\)\n\n\n\\[\n\\Pr(M(x) \\in S) \\leq e^\\varepsilon \\Pr(M(y) \\in S)\n\\]"
  },
  {
    "objectID": "w05/slides.html#first-things-first-nand",
    "href": "w05/slides.html#first-things-first-nand",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "First Things First: NAND",
    "text": "First Things First: NAND\n\nThe irreducible “atom” of computing, it turns out, is the NAND operation: the Negation of the AND operation, which we’ll denote using \\(\\barwedge\\):\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\\(p \\barwedge q\\)\n\n\n\n\n0\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n0\n0\n1\n\n\n1\n1\n1\n0"
  },
  {
    "objectID": "w05/slides.html#why-is-nand-all-we-need",
    "href": "w05/slides.html#why-is-nand-all-we-need",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Why is NAND All We Need?",
    "text": "Why is NAND All We Need?\n\nLogically: any boolean function can be decomposed (e.g., in “conjunctive normal form”) into negation (unary) and conjunction (binary). Here’s both, using only NAND!\n\n\n\n\n\nNegation (\\(\\neg\\))\n\n\n\n\n\n\n\\(p\\)\n\\(\\neg p\\)\n\\(p \\barwedge p\\)\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n\n\n\n\n\n\nConjunction (\\(\\wedge\\))\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\\(p \\barwedge q\\)\n\\(\\left(p \\barwedge q\\right) \\barwedge \\left(p \\barwedge q\\right)\\)\n\n\n\n\n0\n0\n0\n1\n0\n\n\n0\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n1\n1\n1\n0\n1"
  },
  {
    "objectID": "w05/slides.html#or-as-a-bonus",
    "href": "w05/slides.html#or-as-a-bonus",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "OR as a Bonus!",
    "text": "OR as a Bonus!\n\nSince we now have \\(\\neg\\) and \\(\\wedge\\), we can derive \\(\\vee\\)!\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\vee q\\)\n\\(\\neg p\\)\n\\(\\neg q\\)\n\\(\\neg p \\wedge \\neg q\\)\n\\(\\neg\\left(\\neg p \\wedge \\neg q\\right)\\)\n\n\n\n\n0\n0\n0\n1\n1\n1\n0\n\n\n0\n1\n1\n1\n0\n0\n1\n\n\n1\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n\n\n\n\nThis is encapsulated in DeMorgan’s Laws:\n\n\\[\n\\begin{align*}\n\\neg(p \\wedge q) &= \\neg p \\vee \\neg q \\\\\n\\neg(p \\vee q) &= \\neg p \\wedge \\neg q\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w05/slides.html#and-lastly-xor",
    "href": "w05/slides.html#and-lastly-xor",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "And Lastly: XOR",
    "text": "And Lastly: XOR\n\nExclusive or: \\(p \\oplus q\\) = “\\(p\\) or \\(q\\) but not both”\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\oplus q\\)\n\\(p \\vee q\\)\n\\(\\neg(p \\wedge q)\\)\n\\(\\left(p \\vee q) \\wedge \\neg (p \\wedge q\\right)\\)\n\n\n\n\n0\n0\n0\n0\n1\n0\n\n\n0\n1\n1\n1\n1\n1\n\n\n1\n0\n1\n1\n1\n1\n\n\n1\n1\n0\n1\n0\n0"
  },
  {
    "objectID": "w05/slides.html#logic-rightarrow-math",
    "href": "w05/slides.html#logic-rightarrow-math",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Logic \\(\\rightarrow\\) Math",
    "text": "Logic \\(\\rightarrow\\) Math\n\nMathematically: (1) Math can be performed in base 2, and (2) If we have a way to add and multiply in base 2, we have the basic algebraic ring \\(\\mathbb{Z}_2\\) used for “doing math”!\nBut, let’s look at what happens when we do math in \\(\\mathbb{Z}_2\\):\n\n\n\n\n\nAddition (\\(+\\))\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(x +_{\\mathbb{Z}_2} y\\)\n\\(x \\oplus y\\)\n\n\n\n\n0\n0\n0\n0\n\n\n0\n1\n1\n1\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n0\n\n\n\n\n\n\n\n\nMultiplication (\\(\\times\\))\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(x \\times_{\\mathbb{Z}_2} y\\)\n\\(x \\wedge y\\)\n\n\n\n\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\nTakeaway: Mathematical operations in \\(\\mathbb{Z}_2\\) are isomorphic to logical operators (over \\(\\text{T}, \\text{F}\\))!"
  },
  {
    "objectID": "w05/slides.html#implication-for-privacy-preserving-ai",
    "href": "w05/slides.html#implication-for-privacy-preserving-ai",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Implication for Privacy-Preserving AI",
    "text": "Implication for Privacy-Preserving AI\n\nIf we can develop an algorithm allowing addition and multiplication on encrypted values, we can run any AI algorithm we want on the data!\nWe can (a) input encrypted data, then (b) use encrypted-addition and encrypted-multiplication in place of regular addition and multiplication\nExample: in RSA, the most widely-used encryption algorithm, \\(\\mathcal{E}(m) = m^e\\mod{n}\\), so multiplying the encrypted values produces the encrypyed version of the product!\n\n\\[\n\\begin{align*}\n\\mathcal{E}(m_1)\\times \\mathcal{E}(m_2) &= m_1^em_2^e\\mod{n} \\\\\n&= (m_1m_2)^e\\mod{n} \\\\\n&= \\mathcal{E}(m_1 \\times m_2)\n\\end{align*}\n\\]\n\nDoesn’t preserve addition, however:\n\n\\[\n\\begin{align*}\n\\mathcal{E}(m_1) + \\mathcal{E}(m_2) &= m_1^e + m_2^e\\mod{n} \\\\\n\\mathcal{E}(m_1 + m_2) &= (m_1 + m_2)^e\\mod{n}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w05/slides.html#application-zero-knowledge-proofs-and-voting",
    "href": "w05/slides.html#application-zero-knowledge-proofs-and-voting",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Application: Zero-Knowledge Proofs and Voting",
    "text": "Application: Zero-Knowledge Proofs and Voting"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "References",
    "text": "References\n\n\nDworkin, Ronald. 1977. Taking Rights Seriously. A&C Black.\n\n\nRubin, Donald B. 1993. “Statistical Disclosure Limitation.” Journal of Official Statistics 9 (2): 461–68.\n\n\n\n\nDSAN 5450 Week 5: Privacy-Preserving Computation"
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w05/index.html#the-right-to-privacy",
    "href": "w05/index.html#the-right-to-privacy",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "The “Right” to Privacy",
    "text": "The “Right” to Privacy\n\nIn Theory (Normatively): A right is an individual “veto” on a collective decision (Dworkin 1977)\nIn Practice (Descriptively): Rights grow out of the barrel of a gun"
  },
  {
    "objectID": "w05/index.html#all-hope-is-not-lost",
    "href": "w05/index.html#all-hope-is-not-lost",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "All Hope Is Not Lost!",
    "text": "All Hope Is Not Lost!\n\nAnonymity via Synthetic Datasets: if identifying info already exists (e.g., in the US Census), we can generate synthetic datasets which\n\nHave the same distributional properties as the real data, but\nCannot be used to identify you\n\n\\(\\varepsilon\\)-Differential Privacy: Allows us to quantify the tradeoff between accuracy and privacy\nHomomorphic Cryptography makes it possible to encode data in such a way that:\n\nIt can be used to train algorithms, yet\nIt cannot be used to identify you"
  },
  {
    "objectID": "w05/index.html#the-goal",
    "href": "w05/index.html#the-goal",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "The Goal",
    "text": "The Goal\n\nEnable mathematical analysis of a privacy mechanism, via optimization with respect to the accuracy-privacy tradeoff:"
  },
  {
    "objectID": "w05/index.html#pareto-improvements-and-the-pareto-frontier",
    "href": "w05/index.html#pareto-improvements-and-the-pareto-frontier",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Pareto Improvements and the Pareto Frontier",
    "text": "Pareto Improvements and the Pareto Frontier"
  },
  {
    "objectID": "w05/index.html#varepsilon-differential-privacy-definition",
    "href": "w05/index.html#varepsilon-differential-privacy-definition",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "\\(\\varepsilon\\)-Differential Privacy Definition",
    "text": "\\(\\varepsilon\\)-Differential Privacy Definition\n\nA privacy mechanism \\(M\\) provides \\(\\varepsilon\\)-differential privacy if:\n\nFor all pairs of datasets \\(x\\) and \\(y\\) which differ in the data of one person, and\nFor all (probability-theoretic) events \\(S\\)\n\n\n\\[\n\\Pr(M(x) \\in S) \\leq e^\\varepsilon \\Pr(M(y) \\in S)\n\\]"
  },
  {
    "objectID": "w05/index.html#first-things-first-nand",
    "href": "w05/index.html#first-things-first-nand",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "First Things First: NAND",
    "text": "First Things First: NAND\n\nThe irreducible “atom” of computing, it turns out, is the NAND operation: the Negation of the AND operation, which we’ll denote using \\(\\barwedge\\):\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\\(p \\barwedge q\\)\n\n\n\n\n0\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n0\n0\n1\n\n\n1\n1\n1\n0"
  },
  {
    "objectID": "w05/index.html#why-is-nand-all-we-need",
    "href": "w05/index.html#why-is-nand-all-we-need",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Why is NAND All We Need?",
    "text": "Why is NAND All We Need?\n\nLogically: any boolean function can be decomposed (e.g., in “conjunctive normal form”) into negation (unary) and conjunction (binary). Here’s both, using only NAND!\n\n\n\n\n\nNegation (\\(\\neg\\))\n\n\n\n\n\n\n\\(p\\)\n\\(\\neg p\\)\n\\(p \\barwedge p\\)\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n\n\n\n\n\n\nConjunction (\\(\\wedge\\))\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\\(p \\barwedge q\\)\n\\(\\left(p \\barwedge q\\right) \\barwedge \\left(p \\barwedge q\\right)\\)\n\n\n\n\n0\n0\n0\n1\n0\n\n\n0\n1\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n\n\n1\n1\n1\n0\n1"
  },
  {
    "objectID": "w05/index.html#or-as-a-bonus",
    "href": "w05/index.html#or-as-a-bonus",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "OR as a Bonus!",
    "text": "OR as a Bonus!\n\nSince we now have \\(\\neg\\) and \\(\\wedge\\), we can derive \\(\\vee\\)!\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\vee q\\)\n\\(\\neg p\\)\n\\(\\neg q\\)\n\\(\\neg p \\wedge \\neg q\\)\n\\(\\neg\\left(\\neg p \\wedge \\neg q\\right)\\)\n\n\n\n\n0\n0\n0\n1\n1\n1\n0\n\n\n0\n1\n1\n1\n0\n0\n1\n\n\n1\n0\n1\n0\n1\n0\n1\n\n\n1\n1\n1\n0\n0\n0\n1\n\n\n\n\nThis is encapsulated in DeMorgan’s Laws:\n\n\\[\n\\begin{align*}\n\\neg(p \\wedge q) &= \\neg p \\vee \\neg q \\\\\n\\neg(p \\vee q) &= \\neg p \\wedge \\neg q\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w05/index.html#and-lastly-xor",
    "href": "w05/index.html#and-lastly-xor",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "And Lastly: XOR",
    "text": "And Lastly: XOR\n\nExclusive or: \\(p \\oplus q\\) = “\\(p\\) or \\(q\\) but not both”\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\oplus q\\)\n\\(p \\vee q\\)\n\\(\\neg(p \\wedge q)\\)\n\\(\\left(p \\vee q) \\wedge \\neg (p \\wedge q\\right)\\)\n\n\n\n\n0\n0\n0\n0\n1\n0\n\n\n0\n1\n1\n1\n1\n1\n\n\n1\n0\n1\n1\n1\n1\n\n\n1\n1\n0\n1\n0\n0"
  },
  {
    "objectID": "w05/index.html#logic-rightarrow-math",
    "href": "w05/index.html#logic-rightarrow-math",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Logic \\(\\rightarrow\\) Math",
    "text": "Logic \\(\\rightarrow\\) Math\n\nMathematically: (1) Math can be performed in base 2, and (2) If we have a way to add and multiply in base 2, we have the basic algebraic ring \\(\\mathbb{Z}_2\\) used for “doing math”!\nBut, let’s look at what happens when we do math in \\(\\mathbb{Z}_2\\):\n\n\n\n\n\nAddition (\\(+\\))\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(x +_{\\mathbb{Z}_2} y\\)\n\\(x \\oplus y\\)\n\n\n\n\n0\n0\n0\n0\n\n\n0\n1\n1\n1\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n0\n\n\n\n\n\n\n\n\nMultiplication (\\(\\times\\))\n\n\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(x \\times_{\\mathbb{Z}_2} y\\)\n\\(x \\wedge y\\)\n\n\n\n\n0\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\nTakeaway: Mathematical operations in \\(\\mathbb{Z}_2\\) are isomorphic to logical operators (over \\(\\text{T}, \\text{F}\\))!"
  },
  {
    "objectID": "w05/index.html#implication-for-privacy-preserving-ai",
    "href": "w05/index.html#implication-for-privacy-preserving-ai",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Implication for Privacy-Preserving AI",
    "text": "Implication for Privacy-Preserving AI\n\nIf we can develop an algorithm allowing addition and multiplication on encrypted values, we can run any AI algorithm we want on the data!\nWe can (a) input encrypted data, then (b) use encrypted-addition and encrypted-multiplication in place of regular addition and multiplication\nExample: in RSA, the most widely-used encryption algorithm, \\(\\mathcal{E}(m) = m^e\\mod{n}\\), so multiplying the encrypted values produces the encrypyed version of the product!\n\n\\[\n\\begin{align*}\n\\mathcal{E}(m_1)\\times \\mathcal{E}(m_2) &= m_1^em_2^e\\mod{n} \\\\\n&= (m_1m_2)^e\\mod{n} \\\\\n&= \\mathcal{E}(m_1 \\times m_2)\n\\end{align*}\n\\]\n\nDoesn’t preserve addition, however:\n\n\\[\n\\begin{align*}\n\\mathcal{E}(m_1) + \\mathcal{E}(m_2) &= m_1^e + m_2^e\\mod{n} \\\\\n\\mathcal{E}(m_1 + m_2) &= (m_1 + m_2)^e\\mod{n}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w05/index.html#application-zero-knowledge-proofs-and-voting",
    "href": "w05/index.html#application-zero-knowledge-proofs-and-voting",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "Application: Zero-Knowledge Proofs and Voting",
    "text": "Application: Zero-Knowledge Proofs and Voting"
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Anonymity, Privacy-Preserving Computation",
    "section": "References",
    "text": "References\n\n\nDworkin, Ronald. 1977. Taking Rights Seriously. A&C Black.\n\n\nRubin, Donald B. 1993. “Statistical Disclosure Limitation.” Journal of Official Statistics 9 (2): 461–68."
  },
  {
    "objectID": "w03/slides.html#reflective-equilibrium",
    "href": "w03/slides.html#reflective-equilibrium",
    "title": "Week 3: Ethical Frameworks",
    "section": "Reflective Equilibrium",
    "text": "Reflective Equilibrium\n\nMost criticisms of any framework boil down to, “great in theory, but doesn’t work in practice”\nThe way to take this seriously: reflective equilibrium\nIntroduced by Rawls (1951), but popularized by Rawls (1971)\n\n\nFrom Awad et al. (2022)"
  },
  {
    "objectID": "w03/slides.html#rawls-vs.-everyone-else",
    "href": "w03/slides.html#rawls-vs.-everyone-else",
    "title": "Week 3: Ethical Frameworks",
    "section": "Rawls vs. Everyone Else",
    "text": "Rawls vs. Everyone Else\n\nQ: What book had the greatest impact on modern (Anglophone) philosophy?\nOne approach: citations in a widely-used encyclopedia of philosophy1… The results:\n\n\n\n\n\n\n\nBook\n\nBook\n\n\n\n\n1\nRawls 1972, A Theory of Justice (115)\n6\nParfit 1984, Reasons and Persons (62)\n\n\n2\nKripke 1980, Naming and Necessity (88)\n7\nScanlon 1998, What We Owe to Each Other (56)\n\n\n3\nLewis 1986, On the Plurality of Worlds (63)\n8\nWittgenstein 1953, Philosophical Investigations (55)\n\n\n3\nNozick 1974, Anarchy, State and Utopia (63)\n9\nChalmers 1996, The Conscious Mind (53)\n\n\n3\nQuine 1960, Word and Object (63)\n10\nRawls 1993, Political Liberalism (48)\n\n\n\nKeep in mind, however, the dangers of reification we talked about last week!"
  },
  {
    "objectID": "w03/slides.html#easy-mode-descriptive-judgements",
    "href": "w03/slides.html#easy-mode-descriptive-judgements",
    "title": "Week 3: Ethical Frameworks",
    "section": "Easy Mode: Descriptive Judgements",
    "text": "Easy Mode: Descriptive Judgements\nHow did you acquire the concept “red”?\n\nPeople pointed to stuff with certain properties and said “red” (or “rojo” or “红”), as part of constructing an intersubjective communication system\nThese descriptive concepts exist essentially for coordination, like driving on the left vs. right side of the road!\nNothing very profound or difficult seems to come from the commitments implied by this descriptive coordination: “for ease of communication, I’ll vibrate my vocal chords like this (or write these symbols) to indicate \\(x\\), and I’ll vibrate them like this (or write these other symbols) to indicate \\(y\\)” \nOur linguistic choices, when it comes to description, are arbitrary: Our mouths can make these sounds, and each language is a mapping: [combinations of sounds] \\(\\leftrightarrow\\) [things]"
  },
  {
    "objectID": "w03/slides.html#what-makes-ethical-judgements-more-difficult",
    "href": "w03/slides.html#what-makes-ethical-judgements-more-difficult",
    "title": "Week 3: Ethical Frameworks",
    "section": "What Makes Ethical Judgements “More Difficult”?",
    "text": "What Makes Ethical Judgements “More Difficult”?\nHow did you acquire the concept “good”?\n\nPeople pointed to actions/decisions with certain properties and said “good” (and pointed at others and said “bad”), as part of instilling values in you\n“Grass is green” just links two descriptive referents together, while “Honesty is good” takes the descriptive concept “honesty” and links it with the normative concept “good”\nIn doing this, parents/teachers/friends are doing way more than just linking sounds and things in the world: they are also prescribing rules of moral conduct!\nThese normative concepts go beyond communication: the course of your life, the way the future unfolds, is different if you act on one set of norms vs. another\nEthics therefore centrally involves non-arbitrarily-chosen commitments!\ntl;dr: languages are arbitrary conventions for communication, while ethical systems use this language to non-arbitrarily mark out things that are good/bad:\n\nLife would not be very different if we “shuffled” words (we’d just vibrate our vocal chords differently), but would be very different if we “shuffled” good/bad labeling"
  },
  {
    "objectID": "w03/slides.html#historical-example-capitalism-and-the-protestant-ethic",
    "href": "w03/slides.html#historical-example-capitalism-and-the-protestant-ethic",
    "title": "Week 3: Ethical Frameworks",
    "section": "Historical Example: Capitalism and the “Protestant Ethic”",
    "text": "Historical Example: Capitalism and the “Protestant Ethic”\n\nBig changes in history are associated with changes in this good/bad labeling!\nMax Weber (second most-cited sociologist of all time): Protestantism gave rise to the capitalist system by relabeling what things are good vs. bad (Weber 1904):\n\n\n\n\n\nJesus said to his disciples, “Truly, I say to you, only with difficulty will a rich person enter the kingdom of heaven. Again I tell you, it is easier for a camel to go through the eye of a needle than for a rich person to enter the kingdom of God.” (Matthew 19:23-24)\n\n\nOh, were we loving God worthily, we should have no love at all for money! (St. Augustine 1874, pg. 28)\n\n\n\n\nThe earliest capitalists lacked legitimacy in the moral climate in which they found themselves. One of the means they found [to legitimize their behavior] was to appropriate the evaluative vocabulary of Protestantism. (Skinner 2012, pg. 157)"
  },
  {
    "objectID": "w03/slides.html#contemporary-example-palestine",
    "href": "w03/slides.html#contemporary-example-palestine",
    "title": "Week 3: Ethical Frameworks",
    "section": "Contemporary Example: Palestine",
    "text": "Contemporary Example: Palestine\n\nVery few of the relevant empirical facts are in dispute, since opening of crucial archives to three so-called “New Historians” in the 1980s. So why do people still argue?\n\n\n\n\n\nIlan Pappe, one of these historians, concluded from this material that:\n\nThe Israeli state was built upon a massive ethnic cleansing, and\nIs not morally justifiable (Pappe 2006)\n\n\n\nThe immunity Israel has received over the last fifty years encourages others, regimes and oppositions alike, to believe that human and civil rights are irrelevant in the Middle East. The dismantling of the mega-prison in Palestine will send a different, and more hopeful, message.\n\n\n\n\nBenny Morris, another of these historians, concluded that:\n\nThe Israeli state was built upon a massive ethnic cleansing, and\nIs morally justifiable (Morris 1987)\n\n\n\nA Jewish state would not have come into being without the uprooting of 700,000 Palestinians. Therefore it was necessary to uproot them. There was no choice but to expel that population. It was necessary to cleanse the hinterland and cleanse the border areas and cleanse the main roads."
  },
  {
    "objectID": "w03/slides.html#from-w01-promise-keeping",
    "href": "w03/slides.html#from-w01-promise-keeping",
    "title": "Week 3: Ethical Frameworks",
    "section": "(From W01) Promise-Keeping",
    "text": "(From W01) Promise-Keeping\n\nScenario: You just baked a pie, and you promised your friend you’d give them the pie. You’re walking over to the friend’s house to give them the pie.\nSuddenly, you turn the corner to encounter a hostage situation: the hostage-taker is going to kill their hostage unless someone gives them a pie in the next 30 seconds\nDo you give the hostage-taker the pie?\n\n\n\n\n\nConsequentialist Ethics \\(\\implies\\) Yes\n\n\nTo be ethical is to weigh consequences of your actions\nThe positive consequences of giving the pie to the hostage-taker (saving a life) outweigh the negative consequences (breaking your promise to your friend)\n(Ex: Utilitarianism, associated with British philosopher Jeremy Bentham)\n\n\n\n\nDeontological Ethics \\(\\implies\\) No\n\n\nTo be ethical is to live by rules which you would want everyone to follow.\nAs a rule (a “categorical imperative”), you must not break promises. (Breaking this rule \\(\\implies\\) others can also “pick and choose” when to honor promises to you)\n(Ex: Kantian Ethics, associated with German philosopher Immanuel Kant)"
  },
  {
    "objectID": "w03/slides.html#counterargument-to-consequentialism",
    "href": "w03/slides.html#counterargument-to-consequentialism",
    "title": "Week 3: Ethical Frameworks",
    "section": "Counterargument to Consequentialism",
    "text": "Counterargument to Consequentialism\n\n\n\n\nMillions are kept permanently happy, on the one simple condition that a certain lost soul on the far-off edge of things should lead a life of lonely torture (James 1891)\n\n\nModern example: people “out there” suffer so we can have iPhones, etc.\n\n\n\n\n\n\nLe Guin (1973)"
  },
  {
    "objectID": "w03/slides.html#one-solution-individual-rights",
    "href": "w03/slides.html#one-solution-individual-rights",
    "title": "Week 3: Ethical Frameworks",
    "section": "One Solution: Individual Rights",
    "text": "One Solution: Individual Rights\n\n\n\n\nRights are vetoes which individuals can use to cancel out collective/institutional decisions which affect them\nKey example for us: right to privacy\n\n\n\n\n\n\nDworkin (1977)"
  },
  {
    "objectID": "w03/slides.html#counterargument-to-deontology",
    "href": "w03/slides.html#counterargument-to-deontology",
    "title": "Week 3: Ethical Frameworks",
    "section": "Counterargument to Deontology",
    "text": "Counterargument to Deontology\n\nDeontological rule: “Don’t lie”\nReal-world case: Nazis come to your house, ask you if you’re harboring any Jews"
  },
  {
    "objectID": "w03/slides.html#a-synthesis-two-level-utilitarianism",
    "href": "w03/slides.html#a-synthesis-two-level-utilitarianism",
    "title": "Week 3: Ethical Frameworks",
    "section": "A Synthesis: Two-Level Utilitarianism",
    "text": "A Synthesis: Two-Level Utilitarianism\n\n\n\n\nIt would be exhausting to compute Nash equilibrium strategies for every interaction\nInstead, we can develop heuristics that work for most cases, then reevaluate and update when we encounter tough cases\n(Brings us back to reflective equilibrium!)\n\n\n\n\n\n\nKahneman (2011)"
  },
  {
    "objectID": "w03/slides.html#individual-vs.-social-morality",
    "href": "w03/slides.html#individual-vs.-social-morality",
    "title": "Week 3: Ethical Frameworks",
    "section": "Individual vs. Social Morality",
    "text": "Individual vs. Social Morality\n\nIt’s already quite difficult to reason about individual morality\nNow add in the fact that we live in a society 😰\nThings that happen depend not only on our choices but also the choices of others"
  },
  {
    "objectID": "w03/slides.html#enter-game-theory",
    "href": "w03/slides.html#enter-game-theory",
    "title": "Week 3: Ethical Frameworks",
    "section": "Enter Game Theory",
    "text": "Enter Game Theory\n\nA tool for analyzing how individual choices + choices of others \\(\\rightarrow\\) outcomes!\n\n\n\n\n\nExample: You (\\(A\\)) and a friend (\\(B\\)) committed a robbery, and you’re brought into the police station for questioning.\nYou’re placed in separate rooms, and each of you is offered a plea deal: if you testify while your partner stays silent, you go free and they go to jail for 3 years.\nOtherwise, if you both stay silent, they have very little evidence and can only jail you for 1 year\nHowever, there’s a catch: if you both confess, you both get two years in jail, since they now have maximal evidence\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/slides.html#individual-decision-making",
    "href": "w03/slides.html#individual-decision-making",
    "title": "Week 3: Ethical Frameworks",
    "section": "Individual Decision-Making",
    "text": "Individual Decision-Making\n\n\n\n\nLet’s think through \\(A\\)’s best responses to the possible choices \\(B\\) could make:\nIf \\(B\\) stays silent, what is \\(A\\)’s best option?\n\nStaying silent results in 1 year of jail\nTestifying results in 0 years of jail\nSo it is better to testify\n\nIf \\(B\\) testifies, what is \\(A\\)’s best option?\n\nStaying silent results in 3 years of jail\nTestifying results in 2 years of jail\nSo it is better to testify\n\nThe result: regardless of what \\(B\\) does, \\(A\\) is better off testifying!\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/slides.html#the-social-outcome",
    "href": "w03/slides.html#the-social-outcome",
    "title": "Week 3: Ethical Frameworks",
    "section": "The Social Outcome",
    "text": "The Social Outcome\n\n\n\n\nThe game is symmetric, so the same logic applies for \\(B\\)\nConclusion: the outcome of the game will be \\(s^* = (\\text{Testify}, \\text{Testify})\\)\nThis is called a Nash equilibrium: no player can make themselves better off by deviating from this choice\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/slides.html#how-do-we-fix-this-conventions",
    "href": "w03/slides.html#how-do-we-fix-this-conventions",
    "title": "Week 3: Ethical Frameworks",
    "section": "How Do We Fix This? Conventions!",
    "text": "How Do We Fix This? Conventions!\n\nWe encounter this type of problem every day if we drive! You (\\(A\\)) and another driver (\\(B\\)) arrive at an intersection:\n\n\n\n\n\nIf we both stop, we’re mostly bored (\\(u_A = -1\\))\nIf we stop and the other person drives, we’re mad that they got to go and we didn’t (\\(u_A = -3\\))\nIf we both drive, we crash (\\(u_A = -5\\))\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\(-1,-1\\)\n\\(-3,\\phantom{-}0\\)\n\n\nDrive\n\\(\\phantom{-}0, -3\\)\n\\(-5,-5\\)"
  },
  {
    "objectID": "w03/slides.html#without-a-convention",
    "href": "w03/slides.html#without-a-convention",
    "title": "Week 3: Ethical Frameworks",
    "section": "Without A Convention",
    "text": "Without A Convention\n\n\n\n\nWe’re “frozen”: this game has no unique Nash equilibrium, so we cannot say (on the basis of individual rationality) what will happen!\nWithout a convention: power takes over. “War of all against all”, only the strong survive, etc.\nIf my power is \\(p\\) and the other player’s power is \\(q\\), we can model \\(\\Pr((Drive, Stop)) = \\frac{p}{p+q}\\), \\(\\Pr((Stop, Drive)) = \\frac{q}{p+q}\\)\n\\(\\mathbb{E}[u_A] = p(0) + q(-3) = -3\\frac{q}{p+q}\\)\n\\(\\mathbb{E}[u_B] = q(0) + p(-3) = -3\\frac{p}{p+q}\\)\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\({\\color{orange}\\cancel{\\color{black}-1}},{\\color{lightblue}\\cancel{\\color{black}-1}}\\)\n\\(\\boxed{-3},\\boxed{0}\\)\n\n\nDrive\n\\(\\boxed{0}, \\boxed{-3}\\)\n\\({\\color{orange}\\cancel{\\color{black}-5}},{\\color{lightblue}\\cancel{\\color{black}-5}}\\)"
  },
  {
    "objectID": "w03/slides.html#the-convention-of-traffic-lights",
    "href": "w03/slides.html#the-convention-of-traffic-lights",
    "title": "Week 3: Ethical Frameworks",
    "section": "The Convention of Traffic Lights",
    "text": "The Convention of Traffic Lights\n\nIf we don’t want a world where happiness \\(\\propto\\) power, we can introduce traffic lights:\n\n\n\n\n\nNow \\(\\Pr((Drive, Stop)) = 0.5\\), \\(\\Pr((Stop, Drive)) = 0.5\\)\n\\(\\mathbb{E}[u_A] = (0.5)(0) + (0.5)(-3) = -1.5\\)\n\\(\\mathbb{E}[u_B] = (0.5)(-3) + (0.5)(0) = -1.5\\)\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\({\\color{orange}\\cancel{\\color{black}-1}},{\\color{lightblue}\\cancel{\\color{black}-1}}\\)\n\\(\\boxed{-3},\\boxed{0}\\)\n\n\nDrive\n\\(\\boxed{0}, \\boxed{-3}\\)\n\\({\\color{orange}\\cancel{\\color{black}-5}},{\\color{lightblue}\\cancel{\\color{black}-5}}\\)\n\n\n\n\n\n\n\nEmpirical (anthropological) findings across literally thousands of different cultures throughout the world: people are willing to give up rewards to ensure fairness (see, e.g., Henrich et al. (2001))"
  },
  {
    "objectID": "w03/slides.html#so-how-should-we-makechoose-conventions",
    "href": "w03/slides.html#so-how-should-we-makechoose-conventions",
    "title": "Week 3: Ethical Frameworks",
    "section": "So How Should We Make/Choose Conventions?",
    "text": "So How Should We Make/Choose Conventions?\n\nHobbes (1651): Only way out of “war of all against all” is to surrender all power to one sovereign (the Leviathan)\nRousseau (1762): Social contract\nRawls (1971): Social contract behind the “veil of ignorance”\n\nIf we didn’t know where we were going to end up in society, how would we set it up?"
  },
  {
    "objectID": "w03/slides.html#rawls-veil-of-ignorance",
    "href": "w03/slides.html#rawls-veil-of-ignorance",
    "title": "Week 3: Ethical Frameworks",
    "section": "Rawls’ Veil of Ignorance",
    "text": "Rawls’ Veil of Ignorance\n\nProbably the most important tool for policy whitepapers!\n“Justice as fairness” (next week: fairness in AI 😜)\nWe don’t know whether we’ll be \\(A\\) or \\(B\\) in the intersection game, so we’d choose the traffic light!\nMore profoundly: We don’t know what race, gender, class, ethnicity, sexuality, disability status we’ll have; We don’t know whether we’ll be Israeli or Palestinian; we don’t know whether we’ll own means of production or own only our labor power (and thus have to sell it on a market to survive)… 🤔"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Ethical Frameworks",
    "section": "References",
    "text": "References\n\n\nAwad, Edmond, Sydney Levine, Michael Anderson, Susan Leigh Anderson, Vincent Conitzer, M. J. Crockett, Jim A. C. Everett, et al. 2022. “Computational Ethics.” Trends in Cognitive Sciences 26 (5): 388–405. https://doi.org/10.1016/j.tics.2022.02.009.\n\n\nDworkin, Ronald. 1977. Taking Rights Seriously. A&C Black.\n\n\nHenrich, Joseph, Robert Boyd, Samuel Bowles, Colin Camerer, Ernst Fehr, Herbert Gintis, and Richard McElreath. 2001. “In Search of Homo Economicus: Behavioral Experiments in 15 Small-Scale Societies.” American Economic Review 91 (2): 73–78. https://doi.org/10.1257/aer.91.2.73.\n\n\nHobbes, Thomas. 1651. Leviathan: With Selected Variants from the Latin Edition of 1668. Hackett Publishing.\n\n\nJames, William. 1891. “The Moral Philosopher and the Moral Life.” International Journal of Ethics 1 (3): 330–54. https://www.jstor.org/stable/2375309.\n\n\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux.\n\n\nLe Guin, Ursula K. 1973. The Ones Who Walk Away from Omelas: A Story. HarperCollins.\n\n\nMorris, Benny. 1987. The Birth of the Palestinian Refugee Problem, 1947-1949. Cambridge University Press.\n\n\nPappe, Ilan. 2006. The Ethnic Cleansing of Palestine. Simon and Schuster.\n\n\nRawls, John. 1951. “Outline of a Decision Procedure for Ethics.” The Philosophical Review 60 (2): 177–97. https://doi.org/10.2307/2181696.\n\n\n———. 1971. A Theory of Justice: Original Edition. Harvard University Press.\n\n\nRousseau, Jean-Jacques. 1762. The Social Contract. Geneva: J. M. Dent.\n\n\nSkinner, Quentin. 2012. Visions of Politics, Volume 1: Regarding Method. Cambridge: Cambridge University Press.\n\n\nSt. Augustine. 1874. The Works of Aurelius Augustine: Lectures or Tractates on the Gospel According to St. John, v. 2. T. & T. Clark.\n\n\nWeber, Max. 1904. The Protestant Ethic and the Spirit of Capitalism. Courier Corporation.\n\n\n\n\nDSAN 5450 Week 3: Ethical Frameworks"
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Ethical Frameworks",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w03/index.html#reflective-equilibrium",
    "href": "w03/index.html#reflective-equilibrium",
    "title": "Week 3: Ethical Frameworks",
    "section": "Reflective Equilibrium",
    "text": "Reflective Equilibrium\n\nMost criticisms of any framework boil down to, “great in theory, but doesn’t work in practice”\nThe way to take this seriously: reflective equilibrium\nIntroduced by Rawls (1951), but popularized by Rawls (1971)\n\n\n\n\nFrom Awad et al. (2022)"
  },
  {
    "objectID": "w03/index.html#rawls-vs.-everyone-else",
    "href": "w03/index.html#rawls-vs.-everyone-else",
    "title": "Week 3: Ethical Frameworks",
    "section": "Rawls vs. Everyone Else",
    "text": "Rawls vs. Everyone Else\n\nQ: What book had the greatest impact on modern (Anglophone) philosophy?\nOne approach: citations in a widely-used encyclopedia of philosophy1… The results:\n\n\n\n\n\n\n\nBook\n\nBook\n\n\n\n\n1\nRawls 1972, A Theory of Justice (115)\n6\nParfit 1984, Reasons and Persons (62)\n\n\n2\nKripke 1980, Naming and Necessity (88)\n7\nScanlon 1998, What We Owe to Each Other (56)\n\n\n3\nLewis 1986, On the Plurality of Worlds (63)\n8\nWittgenstein 1953, Philosophical Investigations (55)\n\n\n3\nNozick 1974, Anarchy, State and Utopia (63)\n9\nChalmers 1996, The Conscious Mind (53)\n\n\n3\nQuine 1960, Word and Object (63)\n10\nRawls 1993, Political Liberalism (48)"
  },
  {
    "objectID": "w03/index.html#easy-mode-descriptive-judgements",
    "href": "w03/index.html#easy-mode-descriptive-judgements",
    "title": "Week 3: Ethical Frameworks",
    "section": "Easy Mode: Descriptive Judgements",
    "text": "Easy Mode: Descriptive Judgements\nHow did you acquire the concept “red”?\n\nPeople pointed to stuff with certain properties and said “red” (or “rojo” or “红”), as part of constructing an intersubjective communication system\nThese descriptive concepts exist essentially for coordination, like driving on the left vs. right side of the road!\nNothing very profound or difficult seems to come from the commitments implied by this descriptive coordination: “for ease of communication, I’ll vibrate my vocal chords like this (or write these symbols) to indicate \\(x\\), and I’ll vibrate them like this (or write these other symbols) to indicate \\(y\\)” \nOur linguistic choices, when it comes to description, are arbitrary: Our mouths can make these sounds, and each language is a mapping: [combinations of sounds] \\(\\leftrightarrow\\) [things]"
  },
  {
    "objectID": "w03/index.html#what-makes-ethical-judgements-more-difficult",
    "href": "w03/index.html#what-makes-ethical-judgements-more-difficult",
    "title": "Week 3: Ethical Frameworks",
    "section": "What Makes Ethical Judgements “More Difficult”?",
    "text": "What Makes Ethical Judgements “More Difficult”?\nHow did you acquire the concept “good”?\n\nPeople pointed to actions/decisions with certain properties and said “good” (and pointed at others and said “bad”), as part of instilling values in you\n“Grass is green” just links two descriptive referents together, while “Honesty is good” takes the descriptive concept “honesty” and links it with the normative concept “good”\nIn doing this, parents/teachers/friends are doing way more than just linking sounds and things in the world: they are also prescribing rules of moral conduct!\nThese normative concepts go beyond communication: the course of your life, the way the future unfolds, is different if you act on one set of norms vs. another\nEthics therefore centrally involves non-arbitrarily-chosen commitments!\ntl;dr: languages are arbitrary conventions for communication, while ethical systems use this language to non-arbitrarily mark out things that are good/bad:\n\nLife would not be very different if we “shuffled” words (we’d just vibrate our vocal chords differently), but would be very different if we “shuffled” good/bad labeling"
  },
  {
    "objectID": "w03/index.html#historical-example-capitalism-and-the-protestant-ethic",
    "href": "w03/index.html#historical-example-capitalism-and-the-protestant-ethic",
    "title": "Week 3: Ethical Frameworks",
    "section": "Historical Example: Capitalism and the “Protestant Ethic”",
    "text": "Historical Example: Capitalism and the “Protestant Ethic”\n\nBig changes in history are associated with changes in this good/bad labeling!\nMax Weber (second most-cited sociologist of all time): Protestantism gave rise to the capitalist system by relabeling what things are good vs. bad (Weber 1904):\n\n\n\n\n\nJesus said to his disciples, “Truly, I say to you, only with difficulty will a rich person enter the kingdom of heaven. Again I tell you, it is easier for a camel to go through the eye of a needle than for a rich person to enter the kingdom of God.” (Matthew 19:23-24)\n\n\nOh, were we loving God worthily, we should have no love at all for money! (St. Augustine 1874, pg. 28)\n\n\n\n\nThe earliest capitalists lacked legitimacy in the moral climate in which they found themselves. One of the means they found [to legitimize their behavior] was to appropriate the evaluative vocabulary of Protestantism. (Skinner 2012, pg. 157)"
  },
  {
    "objectID": "w03/index.html#contemporary-example-palestine",
    "href": "w03/index.html#contemporary-example-palestine",
    "title": "Week 3: Ethical Frameworks",
    "section": "Contemporary Example: Palestine",
    "text": "Contemporary Example: Palestine\n\nVery few of the relevant empirical facts are in dispute, since opening of crucial archives to three so-called “New Historians” in the 1980s. So why do people still argue?\n\n\n\n\n\nIlan Pappe, one of these historians, concluded from this material that:\n\nThe Israeli state was built upon a massive ethnic cleansing, and\nIs not morally justifiable (Pappe 2006)\n\n\n\nThe immunity Israel has received over the last fifty years encourages others, regimes and oppositions alike, to believe that human and civil rights are irrelevant in the Middle East. The dismantling of the mega-prison in Palestine will send a different, and more hopeful, message.\n\n\n\n\nBenny Morris, another of these historians, concluded that:\n\nThe Israeli state was built upon a massive ethnic cleansing, and\nIs morally justifiable (Morris 1987)\n\n\n\nA Jewish state would not have come into being without the uprooting of 700,000 Palestinians. Therefore it was necessary to uproot them. There was no choice but to expel that population. It was necessary to cleanse the hinterland and cleanse the border areas and cleanse the main roads."
  },
  {
    "objectID": "w03/index.html#from-w01-promise-keeping",
    "href": "w03/index.html#from-w01-promise-keeping",
    "title": "Week 3: Ethical Frameworks",
    "section": "(From W01) Promise-Keeping",
    "text": "(From W01) Promise-Keeping\n\nScenario: You just baked a pie, and you promised your friend you’d give them the pie. You’re walking over to the friend’s house to give them the pie.\nSuddenly, you turn the corner to encounter a hostage situation: the hostage-taker is going to kill their hostage unless someone gives them a pie in the next 30 seconds\nDo you give the hostage-taker the pie?\n\n\n\n\n\nConsequentialist Ethics \\(\\implies\\) Yes\n\n\nTo be ethical is to weigh consequences of your actions\nThe positive consequences of giving the pie to the hostage-taker (saving a life) outweigh the negative consequences (breaking your promise to your friend)\n(Ex: Utilitarianism, associated with British philosopher Jeremy Bentham)\n\n\n\n\nDeontological Ethics \\(\\implies\\) No\n\n\nTo be ethical is to live by rules which you would want everyone to follow.\nAs a rule (a “categorical imperative”), you must not break promises. (Breaking this rule \\(\\implies\\) others can also “pick and choose” when to honor promises to you)\n(Ex: Kantian Ethics, associated with German philosopher Immanuel Kant)"
  },
  {
    "objectID": "w03/index.html#counterargument-to-consequentialism",
    "href": "w03/index.html#counterargument-to-consequentialism",
    "title": "Week 3: Ethical Frameworks",
    "section": "Counterargument to Consequentialism",
    "text": "Counterargument to Consequentialism\n\n\n\n\nMillions are kept permanently happy, on the one simple condition that a certain lost soul on the far-off edge of things should lead a life of lonely torture (James 1891)\n\n\nModern example: people “out there” suffer so we can have iPhones, etc.\n\n\n\n\n\n\nLe Guin (1973)"
  },
  {
    "objectID": "w03/index.html#one-solution-individual-rights",
    "href": "w03/index.html#one-solution-individual-rights",
    "title": "Week 3: Ethical Frameworks",
    "section": "One Solution: Individual Rights",
    "text": "One Solution: Individual Rights\n\n\n\n\nRights are vetoes which individuals can use to cancel out collective/institutional decisions which affect them\nKey example for us: right to privacy\n\n\n\n\n\n\nDworkin (1977)"
  },
  {
    "objectID": "w03/index.html#counterargument-to-deontology",
    "href": "w03/index.html#counterargument-to-deontology",
    "title": "Week 3: Ethical Frameworks",
    "section": "Counterargument to Deontology",
    "text": "Counterargument to Deontology\n\nDeontological rule: “Don’t lie”\nReal-world case: Nazis come to your house, ask you if you’re harboring any Jews"
  },
  {
    "objectID": "w03/index.html#a-synthesis-two-level-utilitarianism",
    "href": "w03/index.html#a-synthesis-two-level-utilitarianism",
    "title": "Week 3: Ethical Frameworks",
    "section": "A Synthesis: Two-Level Utilitarianism",
    "text": "A Synthesis: Two-Level Utilitarianism\n\n\n\n\nIt would be exhausting to compute Nash equilibrium strategies for every interaction\nInstead, we can develop heuristics that work for most cases, then reevaluate and update when we encounter tough cases\n(Brings us back to reflective equilibrium!)\n\n\n\n\n\n\nKahneman (2011)"
  },
  {
    "objectID": "w03/index.html#individual-vs.-social-morality",
    "href": "w03/index.html#individual-vs.-social-morality",
    "title": "Week 3: Ethical Frameworks",
    "section": "Individual vs. Social Morality",
    "text": "Individual vs. Social Morality\n\nIt’s already quite difficult to reason about individual morality\nNow add in the fact that we live in a society 😰\nThings that happen depend not only on our choices but also the choices of others"
  },
  {
    "objectID": "w03/index.html#enter-game-theory",
    "href": "w03/index.html#enter-game-theory",
    "title": "Week 3: Ethical Frameworks",
    "section": "Enter Game Theory",
    "text": "Enter Game Theory\n\nA tool for analyzing how individual choices + choices of others \\(\\rightarrow\\) outcomes!\n\n\n\n\n\nExample: You (\\(A\\)) and a friend (\\(B\\)) committed a robbery, and you’re brought into the police station for questioning.\nYou’re placed in separate rooms, and each of you is offered a plea deal: if you testify while your partner stays silent, you go free and they go to jail for 3 years.\nOtherwise, if you both stay silent, they have very little evidence and can only jail you for 1 year\nHowever, there’s a catch: if you both confess, you both get two years in jail, since they now have maximal evidence\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/index.html#individual-decision-making",
    "href": "w03/index.html#individual-decision-making",
    "title": "Week 3: Ethical Frameworks",
    "section": "Individual Decision-Making",
    "text": "Individual Decision-Making\n\n\n\n\nLet’s think through \\(A\\)’s best responses to the possible choices \\(B\\) could make:\nIf \\(B\\) stays silent, what is \\(A\\)’s best option?\n\nStaying silent results in 1 year of jail\nTestifying results in 0 years of jail\nSo it is better to testify\n\nIf \\(B\\) testifies, what is \\(A\\)’s best option?\n\nStaying silent results in 3 years of jail\nTestifying results in 2 years of jail\nSo it is better to testify\n\nThe result: regardless of what \\(B\\) does, \\(A\\) is better off testifying!\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/index.html#the-social-outcome",
    "href": "w03/index.html#the-social-outcome",
    "title": "Week 3: Ethical Frameworks",
    "section": "The Social Outcome",
    "text": "The Social Outcome\n\n\n\n\nThe game is symmetric, so the same logic applies for \\(B\\)\nConclusion: the outcome of the game will be \\(s^* = (\\text{Testify}, \\text{Testify})\\)\nThis is called a Nash equilibrium: no player can make themselves better off by deviating from this choice\n\n\n\n\n\n\nSource: Wikimedia Commons"
  },
  {
    "objectID": "w03/index.html#how-do-we-fix-this-conventions",
    "href": "w03/index.html#how-do-we-fix-this-conventions",
    "title": "Week 3: Ethical Frameworks",
    "section": "How Do We Fix This? Conventions!",
    "text": "How Do We Fix This? Conventions!\n\nWe encounter this type of problem every day if we drive! You (\\(A\\)) and another driver (\\(B\\)) arrive at an intersection:\n\n\n\n\n\nIf we both stop, we’re mostly bored (\\(u_A = -1\\))\nIf we stop and the other person drives, we’re mad that they got to go and we didn’t (\\(u_A = -3\\))\nIf we both drive, we crash (\\(u_A = -5\\))\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\(-1,-1\\)\n\\(-3,\\phantom{-}0\\)\n\n\nDrive\n\\(\\phantom{-}0, -3\\)\n\\(-5,-5\\)"
  },
  {
    "objectID": "w03/index.html#without-a-convention",
    "href": "w03/index.html#without-a-convention",
    "title": "Week 3: Ethical Frameworks",
    "section": "Without A Convention",
    "text": "Without A Convention\n\n\n\n\nWe’re “frozen”: this game has no unique Nash equilibrium, so we cannot say (on the basis of individual rationality) what will happen!\nWithout a convention: power takes over. “War of all against all”, only the strong survive, etc.\nIf my power is \\(p\\) and the other player’s power is \\(q\\), we can model \\(\\Pr((Drive, Stop)) = \\frac{p}{p+q}\\), \\(\\Pr((Stop, Drive)) = \\frac{q}{p+q}\\)\n\\(\\mathbb{E}[u_A] = p(0) + q(-3) = -3\\frac{q}{p+q}\\)\n\\(\\mathbb{E}[u_B] = q(0) + p(-3) = -3\\frac{p}{p+q}\\)\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\({\\color{orange}\\cancel{\\color{black}-1}},{\\color{lightblue}\\cancel{\\color{black}-1}}\\)\n\\(\\boxed{-3},\\boxed{0}\\)\n\n\nDrive\n\\(\\boxed{0}, \\boxed{-3}\\)\n\\({\\color{orange}\\cancel{\\color{black}-5}},{\\color{lightblue}\\cancel{\\color{black}-5}}\\)"
  },
  {
    "objectID": "w03/index.html#the-convention-of-traffic-lights",
    "href": "w03/index.html#the-convention-of-traffic-lights",
    "title": "Week 3: Ethical Frameworks",
    "section": "The Convention of Traffic Lights",
    "text": "The Convention of Traffic Lights\n\nIf we don’t want a world where happiness \\(\\propto\\) power, we can introduce traffic lights:\n\n\n\n\n\nNow \\(\\Pr((Drive, Stop)) = 0.5\\), \\(\\Pr((Stop, Drive)) = 0.5\\)\n\\(\\mathbb{E}[u_A] = (0.5)(0) + (0.5)(-3) = -1.5\\)\n\\(\\mathbb{E}[u_B] = (0.5)(-3) + (0.5)(0) = -1.5\\)\n\n\n\n\n\n\n\n\n\\(B\\)\n\n\n\n\nStop\nDrive\n\n\n\\(A\\)\nStop\n\\({\\color{orange}\\cancel{\\color{black}-1}},{\\color{lightblue}\\cancel{\\color{black}-1}}\\)\n\\(\\boxed{-3},\\boxed{0}\\)\n\n\nDrive\n\\(\\boxed{0}, \\boxed{-3}\\)\n\\({\\color{orange}\\cancel{\\color{black}-5}},{\\color{lightblue}\\cancel{\\color{black}-5}}\\)\n\n\n\n\n\n\n\nEmpirical (anthropological) findings across literally thousands of different cultures throughout the world: people are willing to give up rewards to ensure fairness (see, e.g., Henrich et al. (2001))"
  },
  {
    "objectID": "w03/index.html#so-how-should-we-makechoose-conventions",
    "href": "w03/index.html#so-how-should-we-makechoose-conventions",
    "title": "Week 3: Ethical Frameworks",
    "section": "So How Should We Make/Choose Conventions?",
    "text": "So How Should We Make/Choose Conventions?\n\nHobbes (1651): Only way out of “war of all against all” is to surrender all power to one sovereign (the Leviathan)\nRousseau (1762): Social contract\nRawls (1971): Social contract behind the “veil of ignorance”\n\nIf we didn’t know where we were going to end up in society, how would we set it up?"
  },
  {
    "objectID": "w03/index.html#rawls-veil-of-ignorance",
    "href": "w03/index.html#rawls-veil-of-ignorance",
    "title": "Week 3: Ethical Frameworks",
    "section": "Rawls’ Veil of Ignorance",
    "text": "Rawls’ Veil of Ignorance\n\nProbably the most important tool for policy whitepapers!\n“Justice as fairness” (next week: fairness in AI 😜)\nWe don’t know whether we’ll be \\(A\\) or \\(B\\) in the intersection game, so we’d choose the traffic light!\nMore profoundly: We don’t know what race, gender, class, ethnicity, sexuality, disability status we’ll have; We don’t know whether we’ll be Israeli or Palestinian; we don’t know whether we’ll own means of production or own only our labor power (and thus have to sell it on a market to survive)… 🤔"
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Ethical Frameworks",
    "section": "References",
    "text": "References\n\n\nAwad, Edmond, Sydney Levine, Michael Anderson, Susan Leigh Anderson, Vincent Conitzer, M. J. Crockett, Jim A. C. Everett, et al. 2022. “Computational Ethics.” Trends in Cognitive Sciences 26 (5): 388–405. https://doi.org/10.1016/j.tics.2022.02.009.\n\n\nDworkin, Ronald. 1977. Taking Rights Seriously. A&C Black.\n\n\nHenrich, Joseph, Robert Boyd, Samuel Bowles, Colin Camerer, Ernst Fehr, Herbert Gintis, and Richard McElreath. 2001. “In Search of Homo Economicus: Behavioral Experiments in 15 Small-Scale Societies.” American Economic Review 91 (2): 73–78. https://doi.org/10.1257/aer.91.2.73.\n\n\nHobbes, Thomas. 1651. Leviathan: With Selected Variants from the Latin Edition of 1668. Hackett Publishing.\n\n\nJames, William. 1891. “The Moral Philosopher and the Moral Life.” International Journal of Ethics 1 (3): 330–54. https://www.jstor.org/stable/2375309.\n\n\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux.\n\n\nLe Guin, Ursula K. 1973. The Ones Who Walk Away from Omelas: A Story. HarperCollins.\n\n\nMorris, Benny. 1987. The Birth of the Palestinian Refugee Problem, 1947-1949. Cambridge University Press.\n\n\nPappe, Ilan. 2006. The Ethnic Cleansing of Palestine. Simon and Schuster.\n\n\nRawls, John. 1951. “Outline of a Decision Procedure for Ethics.” The Philosophical Review 60 (2): 177–97. https://doi.org/10.2307/2181696.\n\n\n———. 1971. A Theory of Justice: Original Edition. Harvard University Press.\n\n\nRousseau, Jean-Jacques. 1762. The Social Contract. Geneva: J. M. Dent.\n\n\nSkinner, Quentin. 2012. Visions of Politics, Volume 1: Regarding Method. Cambridge: Cambridge University Press.\n\n\nSt. Augustine. 1874. The Works of Aurelius Augustine: Lectures or Tractates on the Gospel According to St. John, v. 2. T. & T. Clark.\n\n\nWeber, Max. 1904. The Protestant Ethic and the Spirit of Capitalism. Courier Corporation."
  },
  {
    "objectID": "w03/index.html#footnotes",
    "href": "w03/index.html#footnotes",
    "title": "Week 3: Ethical Frameworks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeep in mind, however, the dangers of reification we talked about last week!↩︎"
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Fairness in AI",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w04/index.html#mathematical-setup",
    "href": "w04/index.html#mathematical-setup",
    "title": "Week 4: Fairness in AI",
    "section": "Mathematical Setup",
    "text": "Mathematical Setup\n\nWe can define notions of fairness under a unified mathematical framework of decision-making\nA decision-maker \\(\\mathscr{D}\\) solves the optimization problem \\(\\max\\{\\mu\\}\\) where, given a set of individuals indexed by \\(i\\), \\[\n\\underbrace{\\mu}_{\\mathclap{\\text{Maximand}}} = \\mathbb{E}_i[\\underbrace{W_i}_{\\mathclap{\\text{Decision}}} \\, \\cdot \\, (\\underbrace{M_i}_{\\mathclap{\\small\\substack{\\text{Merit} \\\\ \\text{(Unobserved)}}}} - \\underbrace{c_i}_{\\mathclap{\\text{Cost}}})]\n\\]\nSince the \\(M_i\\) are unobserved, \\(\\mathscr{D}\\) has to estimate them using a predictive model (e.g., a supervised ML algorithm): \\[\nm(x) = \\mathbb{E}[M \\mid X = x].\n\\]"
  },
  {
    "objectID": "w04/index.html#general-solution",
    "href": "w04/index.html#general-solution",
    "title": "Week 4: Fairness in AI",
    "section": "General Solution",
    "text": "General Solution\n\nWithout any restrictions on the set of admissible policies \\(\\mathscr{W}\\), the solution will satisfy\n\n\\[\nw^*(\\cdot) = \\argmax_{w(\\cdot) \\in \\mathscr{W}}\\mathbb{E}[w(X) \\cdot (m(X) - c)]\n\\]"
  },
  {
    "objectID": "w04/index.html#fair-solution",
    "href": "w04/index.html#fair-solution",
    "title": "Week 4: Fairness in AI",
    "section": "Fair Solution",
    "text": "Fair Solution\n\nDefinitions of fairness under this framework, however, specify a variable \\(A_i\\) for each individual, representing their status with respect to some protected characteristic like race, gender, or disability status\nThen \\(A_i\\) is not allowed to be included in \\(X\\), at a minimum, and different definitions of fairness include additional constraints on \\(\\mathscr{W}\\) based on \\(A_i\\)."
  },
  {
    "objectID": "w04/index.html#predictive-parity",
    "href": "w04/index.html#predictive-parity",
    "title": "Week 4: Fairness in AI",
    "section": "Predictive Parity",
    "text": "Predictive Parity\n\nThe fundamental criterion: for all values \\(a \\in A\\),\n\n\\[\n\\mathbb{E}[M \\mid W = 1, A = a] = \\mathbb{E}[M \\mid W = 1]\n\\]\n\nFor binary \\(A \\in \\{0, 1\\}\\), we can use this criterion to define a parity score \\(\\pi\\):\n\n\\[\n\\pi = \\mathbb{E}[M \\mid W = 1, A = 1] - \\mathbb{E}[M \\mid W = 1, A = 0]\n\\]"
  },
  {
    "objectID": "w04/index.html#implementing-predictive-parity",
    "href": "w04/index.html#implementing-predictive-parity",
    "title": "Week 4: Fairness in AI",
    "section": "Implementing Predictive Parity",
    "text": "Implementing Predictive Parity\n\nEnforcing predictive parity means enforcing that \\(\\pi = 0\\)\nSo, we restrict the set of rules that can be chosen, and solve\n\n\\[\n\\begin{align*}\nw^*(\\cdot) &= \\argmax_{w(\\cdot) \\in \\mathscr{W}_\\text{fair}}\\mathbb{E}[w(X) \\cdot (m(X) - c)], \\\\\n\\mathscr{W}_{\\text{fair}} &= \\{w \\in \\mathscr{W} \\mid \\pi = 0\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w04/index.html#the-solution-choice-only-on-merit",
    "href": "w04/index.html#the-solution-choice-only-on-merit",
    "title": "Week 4: Fairness in AI",
    "section": "The Solution: Choice “Only On Merit”",
    "text": "The Solution: Choice “Only On Merit”\n\nIf we can perfectly predict \\(M\\) (\\(m(X) = M\\)), then\n\n\\[\nw^*(x) = \\begin{cases}\n1 &\\text{if }m(X) &gt; c, \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]\n\nFirms are only being “unfair” if they disciminate in a way that lowers their profits\nImplements definition of discrimination from Becker (1957)\nBut what are the results of this rule? And the results of changing it? And where did \\(M\\) come from?"
  },
  {
    "objectID": "w04/index.html#reflective-equilibrium-affirmative-action",
    "href": "w04/index.html#reflective-equilibrium-affirmative-action",
    "title": "Week 4: Fairness in AI",
    "section": "Reflective Equilibrium: Affirmative Action",
    "text": "Reflective Equilibrium: Affirmative Action\n\nUnder this definition of fairness, affirmative action is unfair:\nPolicy \\(\\alpha\\): Employer is incentivized (via \\(1\\) unit reward) for hiring people with \\(A_i = 1\\)\nPolicy \\(\\beta\\): No incentive for hiring \\(A_i = 1\\) over \\(A_i = 0\\)\n\\(W^*_\\alpha = \\begin{cases}1 &\\text{if }M + A \\geq 1 \\\\ 0 &\\text{otherwise}\\end{cases}, \\; W^*_{\\beta} = M\\)\nFairness scores:\n\n\\[\n\\begin{align*}\n\\pi_\\alpha &= \\mathbb{E}[M \\mid W^*_{\\alpha} = 1, A = 1] - \\mathbb{E}[M \\mid W^*_{\\alpha} = 1, A = 0] = 0.5 - 1 =  -0.5 \\\\\n\\pi_\\beta &= \\mathbb{E}[M \\mid W^*_{\\beta} = 1, A = 1] - \\mathbb{E}[M \\mid W^*_{\\beta} = 1, A = 0] = 1 - 1 = 0\n\\end{align*}\n\\]\n\nSo is fairness the right way to evaluate an algorithm?"
  },
  {
    "objectID": "w04/index.html#affirmative-action-welfare-and-inequality",
    "href": "w04/index.html#affirmative-action-welfare-and-inequality",
    "title": "Week 4: Fairness in AI",
    "section": "Affirmative Action: Welfare and Inequality",
    "text": "Affirmative Action: Welfare and Inequality\n\nNow consider the outcomes from the perspective of the employees rather than the employer: \\(Y(w) = (1 - A) + w\\). Then\n\n\\[\n\\begin{align*}\nY(W_\\alpha) &= 1 + \\begin{cases}1 &\\text{if }A = 0 \\wedge M = 1 \\\\ 0&\\text{otherwise}\\end{cases} \\\\\nY(W_\\beta) &= (1 - A) + M\n\\end{align*}\n\\]\n\nWhich means\n\n\\[\n\\text{Var}[Y(W_\\alpha)] = \\frac{3}{16}, \\text{Var}[Y(W_\\beta)] = \\frac{1}{2}\n\\]"
  },
  {
    "objectID": "w04/index.html#the-impact-of-changes-to-an-algorithm",
    "href": "w04/index.html#the-impact-of-changes-to-an-algorithm",
    "title": "Week 4: Fairness in AI",
    "section": "The Impact of Changes to an Algorithm",
    "text": "The Impact of Changes to an Algorithm\n\nOnce we’ve derived this optimal \\(w^*\\), we can analyze how changes to the algorithm affect outcomes we care about! \\(\\mu\\) and \\(\\pi\\) are objective value as before, but let \\(\\nu\\) represent inequality\nSay we are considering a new algorithm \\(w(x) = w^*(x) + \\varepsilon \\cdot dw(x)\\). Then\n\n\\[\n\\begin{align*}\n\\frac{\\partial \\mu}{\\partial \\varepsilon} &= \\mathbb{E}[dw(X) \\cdot \\ell(X)], \\; \\ell(x) = \\mathbb{E}[M \\mid X = x] - c \\\\\n\\frac{\\partial \\pi}{\\partial \\varepsilon} &= \\mathbb{E}\\left[dw(X) \\cdot p(X)\\right], \\; p(x) = \\mathbb{E}\\mkern-4mu\\left[\\Delta^1\\frac{A}{\\mathbb{E}[WA]} - \\Delta^0\\frac{1-A}{\\mathbb{E}[W(1-A)]} \\; \\middle|\\; X = x\\right], \\\\\n&\\phantom{=} \\; \\Delta^i = M - \\mathbb{E}[M \\mid W = 1, A = i] \\\\\n\\frac{\\partial \\nu}{\\partial \\varepsilon} &= \\mathbb{E}[dw(X) \\cdot n(X)], \\; n(x) = \\mathbb{E}[IF(Y^1, x) - IF(Y^0, x) \\mid X = x]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w04/index.html#who-gets-to-choose-the-objective-function",
    "href": "w04/index.html#who-gets-to-choose-the-objective-function",
    "title": "Week 4: Fairness in AI",
    "section": "Who Gets To Choose The Objective Function?",
    "text": "Who Gets To Choose The Objective Function?\n\nIt turns out that any objective function is an implicit implementation of a weighting of each person’s welfare relative to others"
  },
  {
    "objectID": "w04/index.html#appendix-balance-for-the-positive-class",
    "href": "w04/index.html#appendix-balance-for-the-positive-class",
    "title": "Week 4: Fairness in AI",
    "section": "Appendix: Balance for the Positive Class",
    "text": "Appendix: Balance for the Positive Class\n\nAn alternative to predictive parity, though with similar results\nHere the criterion is, for all \\(i\\),\n\n\\[\n\\mathbb{E}[W_i \\mid M_i = 1, A_i = 1] = \\mathbb{E}[W_i \\mid M_i = 1, A_i = 0]\n\\]"
  },
  {
    "objectID": "w04/slides.html#mathematical-setup",
    "href": "w04/slides.html#mathematical-setup",
    "title": "Week 4: Fairness in AI",
    "section": "Mathematical Setup",
    "text": "Mathematical Setup\n\nWe can define notions of fairness under a unified mathematical framework of decision-making\nA decision-maker \\(\\mathscr{D}\\) solves the optimization problem \\(\\max\\{\\mu\\}\\) where, given a set of individuals indexed by \\(i\\), \\[\n\\underbrace{\\mu}_{\\mathclap{\\text{Maximand}}} = \\mathbb{E}_i[\\underbrace{W_i}_{\\mathclap{\\text{Decision}}} \\, \\cdot \\, (\\underbrace{M_i}_{\\mathclap{\\small\\substack{\\text{Merit} \\\\ \\text{(Unobserved)}}}} - \\underbrace{c_i}_{\\mathclap{\\text{Cost}}})]\n\\]\nSince the \\(M_i\\) are unobserved, \\(\\mathscr{D}\\) has to estimate them using a predictive model (e.g., a supervised ML algorithm): \\[\nm(x) = \\mathbb{E}[M \\mid X = x].\n\\]"
  },
  {
    "objectID": "w04/slides.html#general-solution",
    "href": "w04/slides.html#general-solution",
    "title": "Week 4: Fairness in AI",
    "section": "General Solution",
    "text": "General Solution\n\nWithout any restrictions on the set of admissible policies \\(\\mathscr{W}\\), the solution will satisfy\n\n\\[\nw^*(\\cdot) = \\argmax_{w(\\cdot) \\in \\mathscr{W}}\\mathbb{E}[w(X) \\cdot (m(X) - c)]\n\\]"
  },
  {
    "objectID": "w04/slides.html#fair-solution",
    "href": "w04/slides.html#fair-solution",
    "title": "Week 4: Fairness in AI",
    "section": "Fair Solution",
    "text": "Fair Solution\n\nDefinitions of fairness under this framework, however, specify a variable \\(A_i\\) for each individual, representing their status with respect to some protected characteristic like race, gender, or disability status\nThen \\(A_i\\) is not allowed to be included in \\(X\\), at a minimum, and different definitions of fairness include additional constraints on \\(\\mathscr{W}\\) based on \\(A_i\\)."
  },
  {
    "objectID": "w04/slides.html#predictive-parity",
    "href": "w04/slides.html#predictive-parity",
    "title": "Week 4: Fairness in AI",
    "section": "Predictive Parity",
    "text": "Predictive Parity\n\nThe fundamental criterion: for all values \\(a \\in A\\),\n\n\\[\n\\mathbb{E}[M \\mid W = 1, A = a] = \\mathbb{E}[M \\mid W = 1]\n\\]\n\nFor binary \\(A \\in \\{0, 1\\}\\), we can use this criterion to define a parity score \\(\\pi\\):\n\n\\[\n\\pi = \\mathbb{E}[M \\mid W = 1, A = 1] - \\mathbb{E}[M \\mid W = 1, A = 0]\n\\]"
  },
  {
    "objectID": "w04/slides.html#implementing-predictive-parity",
    "href": "w04/slides.html#implementing-predictive-parity",
    "title": "Week 4: Fairness in AI",
    "section": "Implementing Predictive Parity",
    "text": "Implementing Predictive Parity\n\nEnforcing predictive parity means enforcing that \\(\\pi = 0\\)\nSo, we restrict the set of rules that can be chosen, and solve\n\n\\[\n\\begin{align*}\nw^*(\\cdot) &= \\argmax_{w(\\cdot) \\in \\mathscr{W}_\\text{fair}}\\mathbb{E}[w(X) \\cdot (m(X) - c)], \\\\\n\\mathscr{W}_{\\text{fair}} &= \\{w \\in \\mathscr{W} \\mid \\pi = 0\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w04/slides.html#the-solution-choice-only-on-merit",
    "href": "w04/slides.html#the-solution-choice-only-on-merit",
    "title": "Week 4: Fairness in AI",
    "section": "The Solution: Choice “Only On Merit”",
    "text": "The Solution: Choice “Only On Merit”\n\nIf we can perfectly predict \\(M\\) (\\(m(X) = M\\)), then\n\n\\[\nw^*(x) = \\begin{cases}\n1 &\\text{if }m(X) &gt; c, \\\\\n0 &\\text{otherwise}\n\\end{cases}\n\\]\n\nFirms are only being “unfair” if they disciminate in a way that lowers their profits\nImplements definition of discrimination from Becker (1957)\nBut what are the results of this rule? And the results of changing it? And where did \\(M\\) come from?"
  },
  {
    "objectID": "w04/slides.html#reflective-equilibrium-affirmative-action",
    "href": "w04/slides.html#reflective-equilibrium-affirmative-action",
    "title": "Week 4: Fairness in AI",
    "section": "Reflective Equilibrium: Affirmative Action",
    "text": "Reflective Equilibrium: Affirmative Action\n\nUnder this definition of fairness, affirmative action is unfair:\nPolicy \\(\\alpha\\): Employer is incentivized (via \\(1\\) unit reward) for hiring people with \\(A_i = 1\\)\nPolicy \\(\\beta\\): No incentive for hiring \\(A_i = 1\\) over \\(A_i = 0\\)\n\\(W^*_\\alpha = \\begin{cases}1 &\\text{if }M + A \\geq 1 \\\\ 0 &\\text{otherwise}\\end{cases}, \\; W^*_{\\beta} = M\\)\nFairness scores:\n\n\\[\n\\begin{align*}\n\\pi_\\alpha &= \\mathbb{E}[M \\mid W^*_{\\alpha} = 1, A = 1] - \\mathbb{E}[M \\mid W^*_{\\alpha} = 1, A = 0] = 0.5 - 1 =  -0.5 \\\\\n\\pi_\\beta &= \\mathbb{E}[M \\mid W^*_{\\beta} = 1, A = 1] - \\mathbb{E}[M \\mid W^*_{\\beta} = 1, A = 0] = 1 - 1 = 0\n\\end{align*}\n\\]\n\nSo is fairness the right way to evaluate an algorithm?"
  },
  {
    "objectID": "w04/slides.html#affirmative-action-welfare-and-inequality",
    "href": "w04/slides.html#affirmative-action-welfare-and-inequality",
    "title": "Week 4: Fairness in AI",
    "section": "Affirmative Action: Welfare and Inequality",
    "text": "Affirmative Action: Welfare and Inequality\n\nNow consider the outcomes from the perspective of the employees rather than the employer: \\(Y(w) = (1 - A) + w\\). Then\n\n\\[\n\\begin{align*}\nY(W_\\alpha) &= 1 + \\begin{cases}1 &\\text{if }A = 0 \\wedge M = 1 \\\\ 0&\\text{otherwise}\\end{cases} \\\\\nY(W_\\beta) &= (1 - A) + M\n\\end{align*}\n\\]\n\nWhich means\n\n\\[\n\\text{Var}[Y(W_\\alpha)] = \\frac{3}{16}, \\text{Var}[Y(W_\\beta)] = \\frac{1}{2}\n\\]"
  },
  {
    "objectID": "w04/slides.html#the-impact-of-changes-to-an-algorithm",
    "href": "w04/slides.html#the-impact-of-changes-to-an-algorithm",
    "title": "Week 4: Fairness in AI",
    "section": "The Impact of Changes to an Algorithm",
    "text": "The Impact of Changes to an Algorithm\n\nOnce we’ve derived this optimal \\(w^*\\), we can analyze how changes to the algorithm affect outcomes we care about! \\(\\mu\\) and \\(\\pi\\) are objective value as before, but let \\(\\nu\\) represent inequality\nSay we are considering a new algorithm \\(w(x) = w^*(x) + \\varepsilon \\cdot dw(x)\\). Then\n\n\\[\n\\begin{align*}\n\\frac{\\partial \\mu}{\\partial \\varepsilon} &= \\mathbb{E}[dw(X) \\cdot \\ell(X)], \\; \\ell(x) = \\mathbb{E}[M \\mid X = x] - c \\\\\n\\frac{\\partial \\pi}{\\partial \\varepsilon} &= \\mathbb{E}\\left[dw(X) \\cdot p(X)\\right], \\; p(x) = \\mathbb{E}\\mkern-4mu\\left[\\Delta^1\\frac{A}{\\mathbb{E}[WA]} - \\Delta^0\\frac{1-A}{\\mathbb{E}[W(1-A)]} \\; \\middle|\\; X = x\\right], \\\\\n&\\phantom{=} \\; \\Delta^i = M - \\mathbb{E}[M \\mid W = 1, A = i] \\\\\n\\frac{\\partial \\nu}{\\partial \\varepsilon} &= \\mathbb{E}[dw(X) \\cdot n(X)], \\; n(x) = \\mathbb{E}[IF(Y^1, x) - IF(Y^0, x) \\mid X = x]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w04/slides.html#who-gets-to-choose-the-objective-function",
    "href": "w04/slides.html#who-gets-to-choose-the-objective-function",
    "title": "Week 4: Fairness in AI",
    "section": "Who Gets To Choose The Objective Function?",
    "text": "Who Gets To Choose The Objective Function?\n\nIt turns out that any objective function is an implicit implementation of a weighting of each person’s welfare relative to others"
  },
  {
    "objectID": "w04/slides.html#appendix-balance-for-the-positive-class",
    "href": "w04/slides.html#appendix-balance-for-the-positive-class",
    "title": "Week 4: Fairness in AI",
    "section": "Appendix: Balance for the Positive Class",
    "text": "Appendix: Balance for the Positive Class\n\nAn alternative to predictive parity, though with similar results\nHere the criterion is, for all \\(i\\),\n\n\\[\n\\mathbb{E}[W_i \\mid M_i = 1, A_i = 1] = \\mathbb{E}[W_i \\mid M_i = 1, A_i = 0]\n\\]\n\n\nDSAN 5450 Week 4: Fairness in AI\n\n\n\nBecker, Gary S. 1957. The Economics of Discrimination. University of Chicago Press."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w02/index.html#a-cool-algorithm",
    "href": "w02/index.html#a-cool-algorithm",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "A Cool Algorithm",
    "text": "A Cool Algorithm"
  },
  {
    "objectID": "w02/index.html#training-data-1",
    "href": "w02/index.html#training-data-1",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Training Data",
    "text": "Training Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Gendered Innovations in Science, Health & Medicine, Engineering, and Environment"
  },
  {
    "objectID": "w02/index.html#word-embeddings",
    "href": "w02/index.html#word-embeddings",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n\n\nBolukbasi et al. (2016)"
  },
  {
    "objectID": "w02/index.html#removing-vs.-studying-biases",
    "href": "w02/index.html#removing-vs.-studying-biases",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Removing vs. Studying Biases",
    "text": "Removing vs. Studying Biases\n\n\n\n\n\n\nFrom Kozlowski, Taddy, and Evans (2019)\n\n\n\n\n\n\n\nWordBias: An Interactive Tool for Discovering Intersectional Biases Encoded in Word Embeddings"
  },
  {
    "objectID": "w02/index.html#generating-training-data",
    "href": "w02/index.html#generating-training-data",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Generating Training Data",
    "text": "Generating Training Data"
  },
  {
    "objectID": "w02/index.html#what-comes-with-human-labels-human-biases",
    "href": "w02/index.html#what-comes-with-human-labels-human-biases",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "What Comes With Human Labels? Human Biases!",
    "text": "What Comes With Human Labels? Human Biases!\n\n\n\n\n“Reification”: Pretentious word for an important phenomenon, whereby talking about something (e.g., race) as if it was real ends up leading to it becoming real (having real impacts on people’s lives)1\n\n\nOn average, being classified as a White man as opposed to a Coloured man would have more than quadrupled a person’s income. (Pellicer and Ranchhod 2023)"
  },
  {
    "objectID": "w02/index.html#reification-in-science",
    "href": "w02/index.html#reification-in-science",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Reification in Science",
    "text": "Reification in Science\n\nGoodhart’s Law: “When a measure becomes a target, it ceases to be a good measure”\nCat-and-mouse game between goals and ways of measuring progress towards goals"
  },
  {
    "objectID": "w02/index.html#next-week-working-towards-solutions",
    "href": "w02/index.html#next-week-working-towards-solutions",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Next Week: Working Towards Solutions",
    "text": "Next Week: Working Towards Solutions\n\nThere is a large literature on fairness in AI\nBut, to understand this, we’ll need to understand ethical frameworks!\nRemember: Cannot “prove” \\(q = [\\text{Algorithm is fair}]\\). Only \\(p \\implies q\\), where \\(p\\) is some ethical framework!"
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "References",
    "text": "References\n\n\nBolukbasi, Tolga, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” In Advances in Neural Information Processing Systems. Vol. 29. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html.\n\n\nFields, Barbara J., and Karen E. Fields. 2012. Racecraft: The Soul of Inequality in American Life. Verso Books.\n\n\nKozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.” American Sociological Review 84 (5): 905–49. https://doi.org/10.1177/0003122419877135.\n\n\nPellicer, Miquel, and Vimal Ranchhod. 2023. “Understanding the Effects of Racial Classification in Apartheid South Africa.” Journal of Development Economics 160 (January): 102998. https://doi.org/10.1016/j.jdeveco.2022.102998."
  },
  {
    "objectID": "w02/index.html#footnotes",
    "href": "w02/index.html#footnotes",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFields and Fields (2012), for example, uses the term “racecraft” to describe the reification of blackness in the US↩︎"
  },
  {
    "objectID": "w02/slides.html#a-cool-algorithm",
    "href": "w02/slides.html#a-cool-algorithm",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "A Cool Algorithm",
    "text": "A Cool Algorithm"
  },
  {
    "objectID": "w02/slides.html#training-data-1",
    "href": "w02/slides.html#training-data-1",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Training Data",
    "text": "Training Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Gendered Innovations in Science, Health & Medicine, Engineering, and Environment"
  },
  {
    "objectID": "w02/slides.html#word-embeddings",
    "href": "w02/slides.html#word-embeddings",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\nBolukbasi et al. (2016)"
  },
  {
    "objectID": "w02/slides.html#removing-vs.-studying-biases",
    "href": "w02/slides.html#removing-vs.-studying-biases",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Removing vs. Studying Biases",
    "text": "Removing vs. Studying Biases\n\n\n\n\n\n\nFrom Kozlowski, Taddy, and Evans (2019)\n\n\n\n\n\n\n\nWordBias: An Interactive Tool for Discovering Intersectional Biases Encoded in Word Embeddings"
  },
  {
    "objectID": "w02/slides.html#generating-training-data",
    "href": "w02/slides.html#generating-training-data",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Generating Training Data",
    "text": "Generating Training Data"
  },
  {
    "objectID": "w02/slides.html#what-comes-with-human-labels-human-biases",
    "href": "w02/slides.html#what-comes-with-human-labels-human-biases",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "What Comes With Human Labels? Human Biases!",
    "text": "What Comes With Human Labels? Human Biases!\n\n\n\n\n“Reification”: Pretentious word for an important phenomenon, whereby talking about something (e.g., race) as if it was real ends up leading to it becoming real (having real impacts on people’s lives)1\n\n\nOn average, being classified as a White man as opposed to a Coloured man would have more than quadrupled a person’s income. (Pellicer and Ranchhod 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFields and Fields (2012), for example, uses the term “racecraft” to describe the reification of blackness in the US"
  },
  {
    "objectID": "w02/slides.html#reification-in-science",
    "href": "w02/slides.html#reification-in-science",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Reification in Science",
    "text": "Reification in Science\n\nGoodhart’s Law: “When a measure becomes a target, it ceases to be a good measure”\nCat-and-mouse game between goals and ways of measuring progress towards goals"
  },
  {
    "objectID": "w02/slides.html#next-week-working-towards-solutions",
    "href": "w02/slides.html#next-week-working-towards-solutions",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "Next Week: Working Towards Solutions",
    "text": "Next Week: Working Towards Solutions\n\nThere is a large literature on fairness in AI\nBut, to understand this, we’ll need to understand ethical frameworks!\nRemember: Cannot “prove” \\(q = [\\text{Algorithm is fair}]\\). Only \\(p \\implies q\\), where \\(p\\) is some ethical framework!"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Machine Learning and Training Data",
    "section": "References",
    "text": "References\n\n\nBolukbasi, Tolga, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” In Advances in Neural Information Processing Systems. Vol. 29. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html.\n\n\nFields, Barbara J., and Karen E. Fields. 2012. Racecraft: The Soul of Inequality in American Life. Verso Books.\n\n\nKozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.” American Sociological Review 84 (5): 905–49. https://doi.org/10.1177/0003122419877135.\n\n\nPellicer, Miquel, and Vimal Ranchhod. 2023. “Understanding the Effects of Racial Classification in Apartheid South Africa.” Journal of Development Economics 160 (January): 102998. https://doi.org/10.1016/j.jdeveco.2022.102998.\n\n\n\n\nDSAN 5450 Week 2: Machine Learning and Training Data"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Introduction to the Course",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w01/index.html#axiomatics",
    "href": "w01/index.html#axiomatics",
    "title": "Week 1: Introduction to the Course",
    "section": "Axiomatics",
    "text": "Axiomatics\n\n\n\n\n\nPopular understanding of math: Deals with Facts, statements are true or false\n\nEx: \\(1 + 1 = 2\\) is “true”\n\nReality: No statements in math are absolutely true! Only conditional statements are possible to prove!\nWe cannot prove atomic statements \\(q\\), only implicational statements: \\(p \\implies q\\) for some axiom(s) \\(p\\).\n\n\\(1 + 1 = 2\\) is indeterminate without definitions of \\(1\\), \\(+\\), \\(=\\), and \\(2\\)!\n(Easy counterexample for math/CS majors: \\(1 + 1 = 0\\) in \\(\\mathbb{Z}_2\\))\n\n\n\n\n\n\n\n\nSteingart (2023)"
  },
  {
    "objectID": "w01/index.html#example-1-1-2",
    "href": "w01/index.html#example-1-1-2",
    "title": "Week 1: Introduction to the Course",
    "section": "Example: \\(1 + 1 = 2\\)",
    "text": "Example: \\(1 + 1 = 2\\)\n\n\n\n\nHow it’s taught: this is a rule, and if you don’t follow it you will be banished to eternal hellfire\nHow it’s proved: \\(ZFC \\implies [1 + 1 = 2]\\), where \\(ZFC\\) stands for the Zermelo-Fraenkel Axioms with the Axiom of Choice!\n\n\n\n\n\n\nWhitehead and Russell (1910), p. 83. See here for page in context"
  },
  {
    "objectID": "w01/index.html#proving-1-1-2",
    "href": "w01/index.html#proving-1-1-2",
    "title": "Week 1: Introduction to the Course",
    "section": "Proving \\(1 + 1 = 2\\)",
    "text": "Proving \\(1 + 1 = 2\\)\n(A non-formal proof that still captures the gist:)\n\nAxiom 1: There is a type of thing that can hold other things, which we’ll call a set. We’ll represent it like: \\(\\{ \\langle \\text{\\textit{stuff in the set}} \\rangle \\}\\).\nAxiom 2: Start with the set with nothing in it, \\(\\{\\}\\), and call it “\\(0\\)”.\nAxiom 3: If we put this set \\(0\\) inside of an empty set, we get a new set \\(\\{0\\} = \\{\\{\\}\\}\\), which we’ll call “\\(1\\)”.\nAxiom 4: If we put this set \\(1\\) inside of another set, we get another new set \\(\\{1\\} = \\{\\{\\{\\}\\}\\}\\), which we’ll call “\\(2\\)”.\nAxiom 5: This operation (creating a “next number” by placing a given number inside an empty set) we’ll call succession: \\(S(x) = \\{x\\}\\)\nAxiom 6: We’ll define addition, \\(a + b\\), as applying this succession operation \\(S\\) to \\(a\\), \\(b\\) times. Thus \\(a + b = \\underbrace{S(S(\\cdots (S(}_{b\\text{ times}}a))\\cdots ))\\)\nResult: (Axioms 1-6) \\(\\implies 1 + 1 = S(1) = S(\\{\\{\\}\\}) = \\{\\{\\{\\}\\}\\} = 2. \\; \\blacksquare\\)"
  },
  {
    "objectID": "w01/index.html#how-is-this-relevant-to-ethics",
    "href": "w01/index.html#how-is-this-relevant-to-ethics",
    "title": "Week 1: Introduction to the Course",
    "section": "How Is This Relevant to Ethics?",
    "text": "How Is This Relevant to Ethics?\n(Thank you for bearing with me on that 😅)\n\nJust as mathematicians slowly came to the realization that\n\n\\[\n\\textbf{mathematical results} \\neq \\textbf{(non-implicational) truths}\n\\]\n\nI hope to help you see how\n\n\\[\n\\textbf{ethical conclusions} \\neq \\textbf{(non-implicational) truths}\n\\]\n\nWhen someone says \\(1 + 1 = 2\\), you are allowed to question them, and ask, “On what basis? Please explain…”.\n\nHere the only valid answer is a collection of axioms which entail \\(1 + 1 = 2\\)\n\nWhen someone says Israel has the right to defend itself, you are allowed to question them, and ask, “On what basis? Please explain…”\n\nHere the only valid answer is an ethical framework which entails that Israel has the right to defend itself."
  },
  {
    "objectID": "w01/index.html#axiomatic-systems-statements-can-be-true-and-false",
    "href": "w01/index.html#axiomatic-systems-statements-can-be-true-and-false",
    "title": "Week 1: Introduction to the Course",
    "section": "Axiomatic Systems: Statements Can Be True And False",
    "text": "Axiomatic Systems: Statements Can Be True And False\n\nLet \\(T\\) be the sum of the interior angles of a triangle. We’re taught \\(T = 180^\\circ\\) is a “rule”\nEuclid’s Fifth Postulate \\(P_5\\): Given a line and a point not on it, exactly one line parallel to the given line can be drawn through the point.\n\n\n\n\n\n\n\\(P_5 \\implies T = 180^\\circ\\)\n\n\n(Euclidean Geometry)\n\n\n\n\n\\(\\neg P_5 \\implies T \\neq 180^\\circ\\)\n\n\n(Non-Euclidean Geometry)"
  },
  {
    "objectID": "w01/index.html#ethical-systems-promise-keeping",
    "href": "w01/index.html#ethical-systems-promise-keeping",
    "title": "Week 1: Introduction to the Course",
    "section": "Ethical Systems: Promise-Keeping",
    "text": "Ethical Systems: Promise-Keeping\n\nScenario: You just baked a pie, and you promised your friend you’d give them the pie. You’re walking over to the friend’s house to give them the pie.\nSuddenly, you turn the corner to encounter a hostage situation: the hostage-taker is going to kill their hostage unless someone gives them a pie in the next 30 seconds\nDo you give the hostage-taker the pie?\n\n\n\n\n\nConsequentialist Ethics \\(\\implies\\) Yes\n\n\nTo be ethical is to weigh consequences of your actions\nThe positive consequences of giving the pie to the hostage-taker (saving a life) outweigh the negative consequences (breaking your promise to your friend)\n(Ex: Utilitarianism, associated with British philosopher Jeremy Bentham)\n\n\n\n\nDeontological Ethics \\(\\implies\\) No\n\n\nTo be ethical is to live by rules which you would want everyone to follow.\nAs a rule (a “categorical imperative”), you must not break promises. (Breaking this rule \\(\\implies\\) others can also “pick and choose” when to honor promises to you)\n(Ex: Kantian Ethics, associated with German philosopher Immanuel Kant)"
  },
  {
    "objectID": "w01/index.html#descriptive-vs.-normative",
    "href": "w01/index.html#descriptive-vs.-normative",
    "title": "Week 1: Introduction to the Course",
    "section": "Descriptive vs. Normative",
    "text": "Descriptive vs. Normative\n\n\n\n\n \n  \n \n\n\n\n\n\n\nbin Laden (2005)\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Statement: “Bin Laden attacked us because we had been bombing Iraq for 10 years”\nNormative Statement: “Bin Laden attacked us because we had been bombing Iraq for 10 years, and that is a good justification”\n\n\nDescriptively True (empirically verifiable)\nNormatively True (entailed by axioms + descriptive facts) in some ethical systems, Normatively False (not entailed by axioms + descriptive facts) in others"
  },
  {
    "objectID": "w01/index.html#the-is-ought-distinction",
    "href": "w01/index.html#the-is-ought-distinction",
    "title": "Week 1: Introduction to the Course",
    "section": "The Is-Ought Distinction",
    "text": "The Is-Ought Distinction\n\n\n\n\n\n\nHume on Is vs. Ought (Hume 1739)\n\n\n\n\nthe author proceeds for some time in the ordinary way of reasoning\nsuddenly, instead of the usual copulations of propositions is and is not,\nI meet with no proposition that is not connected with an ought, or an ought not.\nThis change is imperceptible; but is, however, of the last consequence.\n\n\n\n\n\n\nDescriptive (Is)\nNormative (Ought)\n\n\n\n\nGrass is green (true)\nGrass ought to be green (?)\n\n\nGrass is blue (false)\nGrass ought to be blue (?)"
  },
  {
    "objectID": "w01/index.html#what-happens-when-we-confuse-the-two",
    "href": "w01/index.html#what-happens-when-we-confuse-the-two",
    "title": "Week 1: Introduction to the Course",
    "section": "What Happens When We Confuse The Two?",
    "text": "What Happens When We Confuse The Two?\n\n\n\n\nMakes it impossible to “cross the boundary” between your own and others’ beliefs\nCollective welfare: Bad on its own terms (see: wars, racism, etc.)\nSelf-interest: Prevents us from convincing other people of our arguments\n\n\n\n\n\n\nGeertz (1973)"
  },
  {
    "objectID": "w01/index.html#collective-vs.-self-interest",
    "href": "w01/index.html#collective-vs.-self-interest",
    "title": "Week 1: Introduction to the Course",
    "section": "Collective vs. Self-Interest",
    "text": "Collective vs. Self-Interest\n\n\n\n\nGood for collection of people \\(\\; \\nimplies\\) good for each individual person! (😰)\n\\(p\\) = Unions improve everyone’s workplace conditions, whether or not they pay dues\n\\(q\\) = Union dues are voluntary\n\\(p \\wedge q \\implies\\) I can obtain benefits of unions without paying\n\\(\\implies\\) Individually rational to not pay dues\n(Think also about how this applies to climate change policy)\n\n\n\n\n\n\nOlson (1965)"
  },
  {
    "objectID": "w01/index.html#modeling-individual-vs.-societal-outcomes",
    "href": "w01/index.html#modeling-individual-vs.-societal-outcomes",
    "title": "Week 1: Introduction to the Course",
    "section": "Modeling Individual vs. Societal Outcomes",
    "text": "Modeling Individual vs. Societal Outcomes\n\n\n\n\nIndividual Perspective: Individual \\(i\\) chooses whether or not to pay union dues\n\n\n\n\n\n\n\\(\\implies\\) Social Outcome: No Union\n\n\n\n\n\nSchelling (1978)"
  },
  {
    "objectID": "w01/index.html#takeaway-for-policy-whitepapers",
    "href": "w01/index.html#takeaway-for-policy-whitepapers",
    "title": "Week 1: Introduction to the Course",
    "section": "Takeaway for Policy Whitepapers",
    "text": "Takeaway for Policy Whitepapers\n\n\n\n\nYou cannot (just) say, “doing \\(x\\) will be better for society”\nYou must also justify benefits to individuals, or at minimum, the individual organization and its stakeholders!\n(Is this a normative or descriptive claim?)"
  },
  {
    "objectID": "w01/index.html#data-science-for-who",
    "href": "w01/index.html#data-science-for-who",
    "title": "Week 1: Introduction to the Course",
    "section": "Data Science for Who?",
    "text": "Data Science for Who?\n\nWhat are the processes by which data is measured, recorded, and distributed?\n\n\n\n\nThe Library of Missing Datasets. From D’Ignazio and Klein (2020)"
  },
  {
    "objectID": "w01/index.html#example-measuring-freedom-and-human-rights",
    "href": "w01/index.html#example-measuring-freedom-and-human-rights",
    "title": "Week 1: Introduction to the Course",
    "section": "Example: Measuring “Freedom” and “Human Rights”",
    "text": "Example: Measuring “Freedom” and “Human Rights”"
  },
  {
    "objectID": "w01/index.html#operationalization",
    "href": "w01/index.html#operationalization",
    "title": "Week 1: Introduction to the Course",
    "section": "Operationalization",
    "text": "Operationalization\n\n\n\n\nThink about claims commonly made on the basis of “data”:\n\nFree markets cause economic prosperity\nA glass of wine in the evening prevents cancer\nPolicing makes communities safer\n\nHow exactly are “prosperity”, “preventing cancer”, “policing”, “community safety” being measured?\n\n\n\n\n\n\nStiglitz, Sen, and Fitoussi (2010)"
  },
  {
    "objectID": "w01/index.html#what-is-being-compared",
    "href": "w01/index.html#what-is-being-compared",
    "title": "Week 1: Introduction to the Course",
    "section": "What Is Being Compared?",
    "text": "What Is Being Compared?\n\nAre countries with 1 billion people comparable to countries with 10 million people?\nAre countries which were colonized comparable to the colonizing countries?\nWhen did the colonized countries gain independence?\n\n\n\n\nSen, “China and India”"
  },
  {
    "objectID": "w01/index.html#implementation",
    "href": "w01/index.html#implementation",
    "title": "Week 1: Introduction to the Course",
    "section": "Implementation",
    "text": "Implementation\n\n\n\nFrom D’Ignazio and Klein (2020), Ch. 6 (see also)"
  },
  {
    "objectID": "w01/index.html#facial-recognition-algorithms",
    "href": "w01/index.html#facial-recognition-algorithms",
    "title": "Week 1: Introduction to the Course",
    "section": "Facial Recognition Algorithms",
    "text": "Facial Recognition Algorithms\n\n\n\n\n\n\nFacia.ai (2023)\n\n\n\n\n\n\n\nWellcome Collection (1890)\n\n\n\n\n\n\n\n\n\nOuz (2023)\n\n\n\n\n\n\n\nWang and Kosinski (2018)"
  },
  {
    "objectID": "w01/index.html#military-and-police-applications-of-ai",
    "href": "w01/index.html#military-and-police-applications-of-ai",
    "title": "Week 1: Introduction to the Course",
    "section": "Military and Police Applications of AI",
    "text": "Military and Police Applications of AI\n\n\n\n\n\n\nSenor and Singer (2011)\n\n\n\n\n\n\n\nMcNeil (2022)"
  },
  {
    "objectID": "w01/index.html#references",
    "href": "w01/index.html#references",
    "title": "Week 1: Introduction to the Course",
    "section": "References",
    "text": "References\n\n\nbin Laden, Osama. 2005. Messages to the World: The Statements of Osama Bin Laden. Verso Books.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press.\n\n\nFacia.ai. 2023. “Facial Recognition Helps Vendors in Healthcare.” Facia.ai. https://facia.ai/blog/facial-recognition-healthcare/.\n\n\nGeertz, Clifford. 1973. The Interpretation Of Cultures. Basic Books.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nMcNeil, Sam. 2022. “Israel Deploys Remote-Controlled Robotic Guns in West Bank.” AP News, November. https://apnews.com/article/technology-business-israel-robotics-west-bank-cfc889a120cbf59356f5044eb43d5b88.\n\n\nOlson, Mancur. 1965. The Logic of Collective Action. Harvard University Press.\n\n\nOuz. 2023. “Google Pixel 8 Face Unlock Vulnerability Discovered, Allowing Others to Unlock Devices.” Gizmochina. https://www.gizmochina.com/2023/10/16/google-pixel-8-face-unlock/.\n\n\nSchelling, Thomas C. 1978. Micromotives and Macrobehavior. Norton.\n\n\nSenor, Dan, and Saul Singer. 2011. Start-up Nation: The Story of Israel’s Economic Miracle. Grand Central Publishing.\n\n\nSteingart, Alma. 2023. Axiomatics: Mathematical Thought and High Modernism. University of Chicago Press.\n\n\nStiglitz, Joseph E., Amartya Sen, and Jean-Paul Fitoussi. 2010. Mismeasuring Our Lives: Why GDP Doesn’t Add Up. The New Press.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWellcome Collection. 1890. “Composite Photographs: \"The Jewish Type\".” https://wellcomecollection.org/works/ngq29vyw.\n\n\nWhitehead, Alfred North, and Bertrand Russell. 1910. Principia Mathematica. Cambridge University Press."
  },
  {
    "objectID": "w01/slides.html#axiomatics",
    "href": "w01/slides.html#axiomatics",
    "title": "Week 1: Introduction to the Course",
    "section": "Axiomatics",
    "text": "Axiomatics\n\n\n\n\n\nPopular understanding of math: Deals with Facts, statements are true or false\n\nEx: \\(1 + 1 = 2\\) is “true”\n\nReality: No statements in math are absolutely true! Only conditional statements are possible to prove!\nWe cannot prove atomic statements \\(q\\), only implicational statements: \\(p \\implies q\\) for some axiom(s) \\(p\\).\n\n\\(1 + 1 = 2\\) is indeterminate without definitions of \\(1\\), \\(+\\), \\(=\\), and \\(2\\)!\n(Easy counterexample for math/CS majors: \\(1 + 1 = 0\\) in \\(\\mathbb{Z}_2\\))\n\n\n\n\n\n\n\n\nSteingart (2023)"
  },
  {
    "objectID": "w01/slides.html#example-1-1-2",
    "href": "w01/slides.html#example-1-1-2",
    "title": "Week 1: Introduction to the Course",
    "section": "Example: \\(1 + 1 = 2\\)",
    "text": "Example: \\(1 + 1 = 2\\)\n\n\n\n\nHow it’s taught: this is a rule, and if you don’t follow it you will be banished to eternal hellfire\nHow it’s proved: \\(ZFC \\implies [1 + 1 = 2]\\), where \\(ZFC\\) stands for the Zermelo-Fraenkel Axioms with the Axiom of Choice!\n\n\n\n\n\n\nWhitehead and Russell (1910), p. 83. See here for page in context"
  },
  {
    "objectID": "w01/slides.html#proving-1-1-2",
    "href": "w01/slides.html#proving-1-1-2",
    "title": "Week 1: Introduction to the Course",
    "section": "Proving \\(1 + 1 = 2\\)",
    "text": "Proving \\(1 + 1 = 2\\)\n(A non-formal proof that still captures the gist:)\n\nAxiom 1: There is a type of thing that can hold other things, which we’ll call a set. We’ll represent it like: \\(\\{ \\langle \\text{\\textit{stuff in the set}} \\rangle \\}\\).\nAxiom 2: Start with the set with nothing in it, \\(\\{\\}\\), and call it “\\(0\\)”.\nAxiom 3: If we put this set \\(0\\) inside of an empty set, we get a new set \\(\\{0\\} = \\{\\{\\}\\}\\), which we’ll call “\\(1\\)”.\nAxiom 4: If we put this set \\(1\\) inside of another set, we get another new set \\(\\{1\\} = \\{\\{\\{\\}\\}\\}\\), which we’ll call “\\(2\\)”.\nAxiom 5: This operation (creating a “next number” by placing a given number inside an empty set) we’ll call succession: \\(S(x) = \\{x\\}\\)\nAxiom 6: We’ll define addition, \\(a + b\\), as applying this succession operation \\(S\\) to \\(a\\), \\(b\\) times. Thus \\(a + b = \\underbrace{S(S(\\cdots (S(}_{b\\text{ times}}a))\\cdots ))\\)\nResult: (Axioms 1-6) \\(\\implies 1 + 1 = S(1) = S(\\{\\{\\}\\}) = \\{\\{\\{\\}\\}\\} = 2. \\; \\blacksquare\\)"
  },
  {
    "objectID": "w01/slides.html#how-is-this-relevant-to-ethics",
    "href": "w01/slides.html#how-is-this-relevant-to-ethics",
    "title": "Week 1: Introduction to the Course",
    "section": "How Is This Relevant to Ethics?",
    "text": "How Is This Relevant to Ethics?\n(Thank you for bearing with me on that 😅)\n\nJust as mathematicians slowly came to the realization that\n\n\\[\n\\textbf{mathematical results} \\neq \\textbf{(non-implicational) truths}\n\\]\n\nI hope to help you see how\n\n\\[\n\\textbf{ethical conclusions} \\neq \\textbf{(non-implicational) truths}\n\\]\n\nWhen someone says \\(1 + 1 = 2\\), you are allowed to question them, and ask, “On what basis? Please explain…”.\n\nHere the only valid answer is a collection of axioms which entail \\(1 + 1 = 2\\)\n\nWhen someone says Israel has the right to defend itself, you are allowed to question them, and ask, “On what basis? Please explain…”\n\nHere the only valid answer is an ethical framework which entails that Israel has the right to defend itself."
  },
  {
    "objectID": "w01/slides.html#axiomatic-systems-statements-can-be-true-and-false",
    "href": "w01/slides.html#axiomatic-systems-statements-can-be-true-and-false",
    "title": "Week 1: Introduction to the Course",
    "section": "Axiomatic Systems: Statements Can Be True And False",
    "text": "Axiomatic Systems: Statements Can Be True And False\n\nLet \\(T\\) be the sum of the interior angles of a triangle. We’re taught \\(T = 180^\\circ\\) is a “rule”\nEuclid’s Fifth Postulate \\(P_5\\): Given a line and a point not on it, exactly one line parallel to the given line can be drawn through the point.\n\n\n\n\n\n\n\\(P_5 \\implies T = 180^\\circ\\)\n\n\n(Euclidean Geometry)\n\n\n\n\n\\(\\neg P_5 \\implies T \\neq 180^\\circ\\)\n\n\n(Non-Euclidean Geometry)"
  },
  {
    "objectID": "w01/slides.html#ethical-systems-promise-keeping",
    "href": "w01/slides.html#ethical-systems-promise-keeping",
    "title": "Week 1: Introduction to the Course",
    "section": "Ethical Systems: Promise-Keeping",
    "text": "Ethical Systems: Promise-Keeping\n\nScenario: You just baked a pie, and you promised your friend you’d give them the pie. You’re walking over to the friend’s house to give them the pie.\nSuddenly, you turn the corner to encounter a hostage situation: the hostage-taker is going to kill their hostage unless someone gives them a pie in the next 30 seconds\nDo you give the hostage-taker the pie?\n\n\n\n\n\nConsequentialist Ethics \\(\\implies\\) Yes\n\n\nTo be ethical is to weigh consequences of your actions\nThe positive consequences of giving the pie to the hostage-taker (saving a life) outweigh the negative consequences (breaking your promise to your friend)\n(Ex: Utilitarianism, associated with British philosopher Jeremy Bentham)\n\n\n\n\nDeontological Ethics \\(\\implies\\) No\n\n\nTo be ethical is to live by rules which you would want everyone to follow.\nAs a rule (a “categorical imperative”), you must not break promises. (Breaking this rule \\(\\implies\\) others can also “pick and choose” when to honor promises to you)\n(Ex: Kantian Ethics, associated with German philosopher Immanuel Kant)"
  },
  {
    "objectID": "w01/slides.html#descriptive-vs.-normative",
    "href": "w01/slides.html#descriptive-vs.-normative",
    "title": "Week 1: Introduction to the Course",
    "section": "Descriptive vs. Normative",
    "text": "Descriptive vs. Normative\n\n\n\n\n \n  \n \n\n\n\n\n\n\nbin Laden (2005)\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Statement: “Bin Laden attacked us because we had been bombing Iraq for 10 years”\nNormative Statement: “Bin Laden attacked us because we had been bombing Iraq for 10 years, and that is a good justification”\n\n\nDescriptively True (empirically verifiable)\nNormatively True (entailed by axioms + descriptive facts) in some ethical systems, Normatively False (not entailed by axioms + descriptive facts) in others"
  },
  {
    "objectID": "w01/slides.html#the-is-ought-distinction",
    "href": "w01/slides.html#the-is-ought-distinction",
    "title": "Week 1: Introduction to the Course",
    "section": "The Is-Ought Distinction",
    "text": "The Is-Ought Distinction\n\n\n\nHume on Is vs. Ought (Hume 1739)\n\n\n\nthe author proceeds for some time in the ordinary way of reasoning\nsuddenly, instead of the usual copulations of propositions is and is not,\nI meet with no proposition that is not connected with an ought, or an ought not.\nThis change is imperceptible; but is, however, of the last consequence.\n\n\n\n\n\n\n\nDescriptive (Is)\nNormative (Ought)\n\n\n\n\nGrass is green (true)\nGrass ought to be green (?)\n\n\nGrass is blue (false)\nGrass ought to be blue (?)"
  },
  {
    "objectID": "w01/slides.html#what-happens-when-we-confuse-the-two",
    "href": "w01/slides.html#what-happens-when-we-confuse-the-two",
    "title": "Week 1: Introduction to the Course",
    "section": "What Happens When We Confuse The Two?",
    "text": "What Happens When We Confuse The Two?\n\n\n\n\nMakes it impossible to “cross the boundary” between your own and others’ beliefs\nCollective welfare: Bad on its own terms (see: wars, racism, etc.)\nSelf-interest: Prevents us from convincing other people of our arguments\n\n\n\n\n\n\nGeertz (1973)"
  },
  {
    "objectID": "w01/slides.html#collective-vs.-self-interest",
    "href": "w01/slides.html#collective-vs.-self-interest",
    "title": "Week 1: Introduction to the Course",
    "section": "Collective vs. Self-Interest",
    "text": "Collective vs. Self-Interest\n\n\n\n\nGood for collection of people \\(\\; \\nimplies\\) good for each individual person! (😰)\n\\(p\\) = Unions improve everyone’s workplace conditions, whether or not they pay dues\n\\(q\\) = Union dues are voluntary\n\\(p \\wedge q \\implies\\) I can obtain benefits of unions without paying\n\\(\\implies\\) Individually rational to not pay dues\n(Think also about how this applies to climate change policy)\n\n\n\n\n\n\nOlson (1965)"
  },
  {
    "objectID": "w01/slides.html#modeling-individual-vs.-societal-outcomes",
    "href": "w01/slides.html#modeling-individual-vs.-societal-outcomes",
    "title": "Week 1: Introduction to the Course",
    "section": "Modeling Individual vs. Societal Outcomes",
    "text": "Modeling Individual vs. Societal Outcomes\n\n\n\n\nIndividual Perspective: Individual \\(i\\) chooses whether or not to pay union dues\n\n\n\n\n\n\n\\(\\implies\\) Social Outcome: No Union\n\n\n\n\n\nSchelling (1978)"
  },
  {
    "objectID": "w01/slides.html#takeaway-for-policy-whitepapers",
    "href": "w01/slides.html#takeaway-for-policy-whitepapers",
    "title": "Week 1: Introduction to the Course",
    "section": "Takeaway for Policy Whitepapers",
    "text": "Takeaway for Policy Whitepapers\n\n\n\n\nYou cannot (just) say, “doing \\(x\\) will be better for society”\nYou must also justify benefits to individuals, or at minimum, the individual organization and its stakeholders!\n(Is this a normative or descriptive claim?)"
  },
  {
    "objectID": "w01/slides.html#data-science-for-who",
    "href": "w01/slides.html#data-science-for-who",
    "title": "Week 1: Introduction to the Course",
    "section": "Data Science for Who?",
    "text": "Data Science for Who?\n\nWhat are the processes by which data is measured, recorded, and distributed?\n\n\nThe Library of Missing Datasets. From D’Ignazio and Klein (2020)"
  },
  {
    "objectID": "w01/slides.html#example-measuring-freedom-and-human-rights",
    "href": "w01/slides.html#example-measuring-freedom-and-human-rights",
    "title": "Week 1: Introduction to the Course",
    "section": "Example: Measuring “Freedom” and “Human Rights”",
    "text": "Example: Measuring “Freedom” and “Human Rights”"
  },
  {
    "objectID": "w01/slides.html#operationalization",
    "href": "w01/slides.html#operationalization",
    "title": "Week 1: Introduction to the Course",
    "section": "Operationalization",
    "text": "Operationalization\n\n\n\n\nThink about claims commonly made on the basis of “data”:\n\nFree markets cause economic prosperity\nA glass of wine in the evening prevents cancer\nPolicing makes communities safer\n\nHow exactly are “prosperity”, “preventing cancer”, “policing”, “community safety” being measured?\n\n\n\n\n\n\nStiglitz, Sen, and Fitoussi (2010)"
  },
  {
    "objectID": "w01/slides.html#what-is-being-compared",
    "href": "w01/slides.html#what-is-being-compared",
    "title": "Week 1: Introduction to the Course",
    "section": "What Is Being Compared?",
    "text": "What Is Being Compared?\n\nAre countries with 1 billion people comparable to countries with 10 million people?\nAre countries which were colonized comparable to the colonizing countries?\nWhen did the colonized countries gain independence?\n\n\nSen, “China and India”"
  },
  {
    "objectID": "w01/slides.html#implementation",
    "href": "w01/slides.html#implementation",
    "title": "Week 1: Introduction to the Course",
    "section": "Implementation",
    "text": "Implementation\n\nFrom D’Ignazio and Klein (2020), Ch. 6 (see also)"
  },
  {
    "objectID": "w01/slides.html#facial-recognition-algorithms",
    "href": "w01/slides.html#facial-recognition-algorithms",
    "title": "Week 1: Introduction to the Course",
    "section": "Facial Recognition Algorithms",
    "text": "Facial Recognition Algorithms\n\n\n\n\n\n\nFacia.ai (2023)\n\n\n\n\n\n\n\nWellcome Collection (1890)\n\n\n\n\n\n\n\n\n\nOuz (2023)\n\n\n\n\n\n\n\nWang and Kosinski (2018)"
  },
  {
    "objectID": "w01/slides.html#military-and-police-applications-of-ai",
    "href": "w01/slides.html#military-and-police-applications-of-ai",
    "title": "Week 1: Introduction to the Course",
    "section": "Military and Police Applications of AI",
    "text": "Military and Police Applications of AI\n\n\n\n\n\n\nSenor and Singer (2011)\n\n\n\n\n\n\n\nMcNeil (2022)"
  },
  {
    "objectID": "w01/slides.html#references",
    "href": "w01/slides.html#references",
    "title": "Week 1: Introduction to the Course",
    "section": "References",
    "text": "References\n\n\nbin Laden, Osama. 2005. Messages to the World: The Statements of Osama Bin Laden. Verso Books.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press.\n\n\nFacia.ai. 2023. “Facial Recognition Helps Vendors in Healthcare.” Facia.ai. https://facia.ai/blog/facial-recognition-healthcare/.\n\n\nGeertz, Clifford. 1973. The Interpretation Of Cultures. Basic Books.\n\n\nHume, David. 1739. A Treatise of Human Nature: Being an Attempt to Introduce the Experimental Method of Reasoning Into Moral Subjects; and Dialogues Concerning Natural Religion. Longmans, Green.\n\n\nMcNeil, Sam. 2022. “Israel Deploys Remote-Controlled Robotic Guns in West Bank.” AP News, November. https://apnews.com/article/technology-business-israel-robotics-west-bank-cfc889a120cbf59356f5044eb43d5b88.\n\n\nOlson, Mancur. 1965. The Logic of Collective Action. Harvard University Press.\n\n\nOuz. 2023. “Google Pixel 8 Face Unlock Vulnerability Discovered, Allowing Others to Unlock Devices.” Gizmochina. https://www.gizmochina.com/2023/10/16/google-pixel-8-face-unlock/.\n\n\nSchelling, Thomas C. 1978. Micromotives and Macrobehavior. Norton.\n\n\nSenor, Dan, and Saul Singer. 2011. Start-up Nation: The Story of Israel’s Economic Miracle. Grand Central Publishing.\n\n\nSteingart, Alma. 2023. Axiomatics: Mathematical Thought and High Modernism. University of Chicago Press.\n\n\nStiglitz, Joseph E., Amartya Sen, and Jean-Paul Fitoussi. 2010. Mismeasuring Our Lives: Why GDP Doesn’t Add Up. The New Press.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWellcome Collection. 1890. “Composite Photographs: \"The Jewish Type\".” https://wellcomecollection.org/works/ngq29vyw.\n\n\nWhitehead, Alfred North, and Bertrand Russell. 1910. Principia Mathematica. Cambridge University Press.\n\n\n\n\nDSAN 5450 Week 1: Introduction"
  }
]